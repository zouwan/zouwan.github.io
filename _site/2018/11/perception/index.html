<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <title>Perception</title>
  <meta name="description" content="        1. 什么是感知机(Perception)？  感知机(perceptron)是二类分类的线性分类模型,其输入为实例的特征向量,输出为实例的类别,取+1和-1二值.          感知机对应于输入空间中将实例划分为正负两类的分离超平面，属于 判别模型。      感知机学习旨在求出将训练数据进...">
  <meta name="author" content="Wei Wang">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Perception">
  <meta name="twitter:description" content="        1. 什么是感知机(Perception)？  感知机(perceptron)是二类分类的线性分类模型,其输入为实例的特征向量,输出为实例的类别,取+1和-1二值.          感知机对应于输入空间中将实例划分为正负两类的分离超平面，属于 判别模型。      感知机学习旨在求出将训练数据进...">
  
  <meta property="og:type" content="article">
  <meta property="og:title" content="Perception">
  <meta property="og:description" content="        1. 什么是感知机(Perception)？  感知机(perceptron)是二类分类的线性分类模型,其输入为实例的特征向量,输出为实例的类别,取+1和-1二值.          感知机对应于输入空间中将实例划分为正负两类的分离超平面，属于 判别模型。      感知机学习旨在求出将训练数据进...">
  
  <link rel="icon" type="image/png" href="/assets/images/favicon.png" />
  <link href="/assets/images/favicon.png" rel="shortcut icon" type="image/png">
  
  <link rel="stylesheet" href="/css/main.css">
  <link href="//netdna.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">

  <link rel="canonical" href="/2018/11/perception/">
  <link rel="alternate" type="application/rss+xml" title="噢!乖" href="/feed.xml">
  
  <meta name="google-site-verification" content="1-1ZlHoRvM0T2FqPbW2S-qLgYXN6rsn52kErlMPd_gw" />
  
</head>


  <body>

    <span class="mobile btn-mobile-menu">
        <i class="fa fa-list btn-mobile-menu__icon"></i>
        <i class="fa fa-angle-up btn-mobile-close__icon hidden"></i>
    </span>
    
    <header class="panel-cover panel-cover--collapsed" style="background-image: url('/assets/images/background-cover.jpg')">
  <div class="panel-main">

    <div class="panel-main__inner panel-inverted">
    <div class="panel-main__content">

        <a href="/#blog" title="前往 噢!乖 的主页" class="blog-button"><img src="/assets/images/avatar.jpg" width="80" alt="噢!乖 logo" class="panel-cover__logo logo" /></a>
        <h1 class="panel-cover__title panel-title"><a href="/#blog" title="link to homepage for 噢!乖" class="blog-button">噢!乖</a></h1>
        
        <span class="panel-cover__subtitle panel-subtitle">乖乖</span>
        
        <hr class="panel-cover__divider" />
        <p class="panel-cover__description"></p>
        <hr class="panel-cover__divider panel-cover__divider--secondary" />
        
        
        
        <div class="navigation-wrapper">
          <div>
            <nav class="cover-navigation cover-navigation--primary">
              <ul class="navigation">
                <li class="navigation__item"><a href="/#blog" title="Visit blog" class="blog-button">Blog</a></li>
                
              </ul>
            </nav>
          </div>
          
          <div><nav class="cover-navigation navigation--social">
  <ul class="navigation">

  

  
  <!-- Github -->
  <li class="navigation__item">
    <a href="https://github.com/zouwan" title="@zouwan 的 Github" target="_blank">
      <i class='social fa fa-github'></i>
      <span class="label">Github</span>
    </a>
  </li>
  
  
  

  

  <!-- RSS -->
  <li class="navigation__item">
    <a href="/feed.xml" rel="author" title="RSS" target="_blank">
      <i class='social fa fa-rss'></i>
      <span class="label">RSS</span>
    </a>
  </li>

  
  <!-- Email -->
  <li class="navigation__item">
    <a href="mailto:zouwan@gmail.com" title="Contact me">
      <i class='social fa fa-envelope'></i>
      <span class="label">Email</span>
    </a>
  </li>
  

  </ul>
</nav>
</div>
        </div>
      </div>
    </div>
    
    
    <div class="panel-cover--overlay cover-slate"></div>
    
  </div>
</header>


    <div class="content-wrapper">
        <div class="content-wrapper__inner">
            <article class="post-container post-container--single" itemscope itemtype="http://schema.org/BlogPosting">
  <header class="post-header">
    <div class="post-meta">
      <time datetime="2018-11-25 00:00:00 +0800" itemprop="datePublished" class="post-meta__date date">2018-11-25</time> &#8226; <span class="post-meta__tags tags"></span>
    </div>
    <h1 class="post-title">Perception</h1>
  </header>

  <section class="post">
    <head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>
<h3 id="1-什么是感知机perception">1. 什么是感知机(Perception)？</h3>
<ol>
  <li><b>感知机</b>(perceptron)是二类分类的线性分类模型,其输入为实例的特征向量,输出为实例的类别,取+1和-1二值.
    <ul>
      <li>感知机对应于输入空间中将实例划分为正负两类的分离超平面，属于 <b>判别模型</b>。</li>
      <li>感知机学习旨在求出将训练数据进行线性划分的分离超平面，为此导入了基于误分类的损失函数，利用梯度下降法对损失函数进行极小化，求得感知机模型。</li>
      <li>感知机学习算法具有简单而易于实现的优点，分为原始形式和对偶形式。</li>
      <li>感知机是神经网络与支持向量机的基础。</li>
    </ul>
  </li>
</ol>

<h3 id="2-感知机模型原理">2. 感知机模型原理</h3>
<p>感知机满足输入空间到输出空间的如下函数称为感知机：
    <script type="math/tex">f(x)=sign(wx+b)</script> 其中 w 称为权值(weight，b 称为偏置(bias), sign为激活函数
    <script type="math/tex">% <![CDATA[
sign(x)=\begin{cases} +1,x>0 \\ -1,x<0 \\ \end{cases} %]]></script>
   sign(x) 将大于0的分为1， 小于0的分为-1.</p>
<ol>
  <li>感知机相比逻辑回归主要区别是 <b>激活函数不同</b>
    <ol>
      <li>感知机激活函数是 sign(x),又称单位阶跃函数</li>
      <li>逻辑回归（logistic regression)激活函数是 sigmoid</li>
      <li>sigmoid一般取阈值0.5， 大于0.5的分为1， 小于0.1的分为0</li>
      <li>逻辑回归也被看做是一种概率估计， 详见逻辑回归(todo)</li>
    </ol>
  </li>
  <li>工作原理
    <ol>
      <li>给每一个属性一个权重，对属性和权重乘积求和得到的结果和一个阈值进行比较，可以输出一个二分类结果。比如判定是否能够给张三放贷。</li>
    </ol>
  </li>
  <li>感知机的几何解释:
    <ol>
      <li>感知机可以用线性方程表示；
 <script type="math/tex">w \cdot x+b=0</script></li>
      <li>几何意义如下图:
 对应于特征空间Rn中的一个超平面S，其中w是超平面的法向量，b是截距。这个超平面将特征空间划分为两个部分，位于两部分的点分别被分为正、负两类。因此，超平面S称为分离超平面（separating hyperplanes）
 <img src="https://pic2.zhimg.com/80/v2-7bd3d267a3d50a511b4b51aace348301_hd.png" alt="image" /></li>
      <li>啥？超平面 （黑脸问号)
        <ol>
          <li>超平面在二维空间里就是直线，方程是a<em>x+b</em>y+c=0</li>
          <li>超平面在三维空间里就是平面，方程是a<em>x+b</em>y+c*z+d=0</li>
          <li>在n维空间里推广就是就是a<em>x+b</em>y+c*z+……..+k=0
(a,b,c…)对应向量w, 是由平面确定的数，
   （x,y,z..)是平面上任一点的坐标，对应方程的向量x</li>
        </ol>
      </li>
      <li>假定 $wx+b=0$是一个超平面，令$g(x)=wx+b$,即超平面上的所有点都满足$g(x)=0$. 对于超平面的一侧点满足 $g(x)&gt;0$, 另一侧满足 $g(x)&lt;0$.</li>
      <li>对于不在超平面上的点x到超平面的距离是 $ r=\frac {g(x)}{\left|w \right|}$
 证明：见下图，其中 O 为原点， $X_p$为超平面上的点， X 是超平面外的点， w 为超平面法向量
<img src="https://img-blog.csdn.net/20171015173343056?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSmFzdGVyX3dpc2RvbQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="image" /></li>
    </ol>

    <p>(1) $X=X_p+r * \frac{w}{ \left|w\right|}$</p>
    <blockquote>
      <p>向量基本运算法则：$ OX = OX_p + X_pX$; w为法向量，所以 $\frac{w}{\left|w\right|}$ 是垂直于超平面的单位向量</p>
    </blockquote>

    <p>(2) $g(x) = w^T(x_p+r\frac{w}{\left|w\right|})+w_0 = (w^T * x_p + w_0) + r * \frac{w^T * w}{\left|w\right|} = r * \left| w \right|$</p>
    <blockquote>
      <p>等式（1）带入等式 $g(x)=wx+b$, 由于$X_p$在超平面， 所以 $g(X_p) = w^T * x_p + w_0 = 0$</p>
    </blockquote>

    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code> 结论一得证
</code></pre></div>    </div>
  </li>
</ol>

<h3 id="3-感知机学习目标">3. 感知机学习目标</h3>
<p>  学习参数w与b，确定了w与b，图上的直线（高维空间下为超平面）也就确定了; 感知机的学习目标是找到损失函数,转化为最优化问题</p>
<ol>
  <li>思路一：误分类点数目
    <ol>
      <li>无法函数化表示</li>
      <li>w, b 不是连续可导</li>
      <li>由于上述1，2两点，思路一不可行，考虑思路二</li>
    </ol>
  </li>
  <li>思路二：误分类点距离超平面距离，总距离越小越好
 <script type="math/tex">\frac{1}{\| w\|} | w \cdot x_0 + b |</script>
    <ol>
      <li>当数据点被误分类(+1吧被误判为-1), 则 $(w \cdot x_0 + b) &lt; 0 $, 此时预测值 $y_i$=+1, 满足 <script type="math/tex">-y_i(w \cdot x_0 + b) > 0</script></li>
      <li>当数据点被误分类(-1吧被误判为+1), 则 $(w \cdot x_0 + b) &gt; 0 $, 此时预测值 $y_i$=-1, 满足 <script type="math/tex">y_i(w \cdot x_0 + b) > 0</script></li>
    </ol>
  </li>
  <li>损失函数确定
    <ol>
      <li>
        <p>上式不考虑$|w|$, 去掉后面的绝对值，得到损失函数:
 <script type="math/tex">J(w,b) = -\sum_{x_i\in M}{y_i(w \cdot x_i + b)}，M为误分类点集</script></p>
      </li>
      <li>
        <p>感知机不关心实际超平面距离每个点的距离（定量，误分类点的距离），只关心最终分类是否正确（定性，误分类点的个数）</p>
      </li>
    </ol>
  </li>
  <li>
    <p>损失函数推导：
把正例预测为负例： $w<em>x_i+b&gt;0时， y_i=-1$
把负例预测为正例： $w</em>x_i+b&lt;0时， y_i=1$
可以得到误分类点到超平面总的距离是（注意， 只考虑不在超平面的点）
<script type="math/tex">-\frac{1}{\left\|w\right\|}\sum_{x_i \in M}{y_i*(w*x_i+b)}</script> <script type="math/tex">-\sum_{x_i \in M}{y_i*(w*x_i+b)}</script></p>
  </li>
  <li>更新策略（梯度下降）
    <ol>
      <li>对损失函数计算梯度（梯度下降）
<script type="math/tex">\left\{ \begin{array}{lr} \nabla_{w}{L(w,b)} = -\sum_{x \in M}{y_i * x_i} \\ \nabla_{b}{L(w,b)} = -\sum_{x \in M}{y_i} \end{array} \right.</script><br />
这里使用随机梯度（随机选取误分类点$(x_i, y_i)$对 w, b 进行更新， 更新公式如下：
<script type="math/tex">\left\{ \begin{array}{lr} w \leftarrow w+\eta y_i*x_i \\ b \leftarrow b+\eta y_i  \end{array} \right.</script></li>
      <li>如何理解上面更新公式？(from <a href="https://www.zhihu.com/question/57747902">知乎</a>)
        <ol>
          <li>梯度更新公式确实不是推导而是创造出来的，所以只能从概念上去理解</li>
          <li>设想下有个函数，你的目标是：找到一个参数$\theta$ 使得它的值$Y$最小。但它很复杂，你无法找到这个参数的解析解，所以你希望通过梯度下降法去猜这个参数</li>
          <li>问题是怎么猜？
 对于多数有连续性的函数来说，显然不可能把每个$\theta$都试一遍。所以只能先随机取一个$\theta$，然后看看怎么调整它最有可能使得$\theta$变小。
 把这个过程重复n遍，自然最后得到的损失函数L的估值会越来越小</li>
          <li>调整策略（参考文章:[梯度下降法]）
 基于当前我们拥有的那个参数$\theta$，所以有了：<script type="math/tex">\theta_{t+1}=\theta_t + \Delta</script>那现在问题是每次更新的时候这个$\theta$应该取什么值？
 我们知道关于某变量的（偏）导数的概念是指当（仅仅）该变量往 <b>正向</b> 的变化量趋向于0时的其函数值变化量的极限。若求Y关于$\theta_t$的导数，得到一个值比如+5, 那就说明若现在我们把$\theta_t$往负向移动，损失函数 $J(w,b）$的值会变小，但不一定是正好-5。同理若现在导数是-5，那么把$\theta$往 <b>负向</b>移动，损失函数$J(w,b)$值会变小。<br />
 不管导数值$\theta$是正的还是负的(正负即导数的方向), 对于$J(w,b)$来说，$-\theta$的最终方向（即最终的正负号，决定是增(+)还是减(-))一定是能将$J(w,b)$值变小的方向（除非导数为0）。所以有了:
 <script type="math/tex">\theta_{t+1}=\theta_t + (-\Delta)</script>但是说到底，$\Delta$的绝对值只是个关于Y的变化率，本质上和$\theta_t$没关系。所以为了抹去$\Delta$在幅度上对$\theta_t$的影响，需要一个学习率来控制：$\alpha$。所以有了：$\alpha \in \left(0,1\right]$
 <script type="math/tex">\theta_{t+1}=\theta_t + (-\alpha \Delta) =\theta_t -\alpha \Delta</script>就是有多少个参数，就有多少个不同的$\Delta$。
 <br />现在分析在梯度下降法中最常听到的一句话：“梯度下降法就是朝着梯度的反方向迭代地调整参数直到收敛。” 这里的梯度就是 $\Delta$ ,而梯度的反方向就是 $-\Delta$ 的符号方向—梯度实际上是个向量。所以这个角度来说，即使我们只有一个参数需要调整，也可以认为它是个一维的向量。 
 &lt;br<b>整个过程你可以想象自己站在一个山坡上，准备走到山脚下（最小值的地方），于是很自然地你会考虑朝着哪个方向走，方向由$\Delta$的方向给出，而至于一次走多远，由$|\alpha\Delta|$来控制。这种方式相信你应该能理解其只能找到局部最小值，而不是全局的。</b></li>
          <li>换个角度看，想象损失函数是$y=x^2$的抛物线形式，其梯度为 $y’=2x$
            <ol>
              <li>纵坐标y表示损失函数，横坐标x表示参数 $\theta$</li>
              <li>$\theta$在最低点左边时，导数小于0，只有$\theta$往右边（最低点方向）走，损失函数才会变小，即$\theta$要往导数的负方向变化
                <ol>
                  <li>当 x=-2; y=-4, 此时往负向损失函数$y$增大，往正向走损失函数$y$减少</li>
                </ol>
              </li>
              <li>$\theta$在最低点右边时，导数大于0，只有$\theta$往左边（最低点方向）走，损失函数才会变小，即$\theta$要往导数的负方向变化
 1.当 x=+2; y=+4, 此时往正向损失函数$y$增大，往负向走损失函数$y$减少          <br />
 <img src="/assets/images//15432151219295.jpg" alt="-w450" />}}</li>
            </ol>
          </li>
        </ol>
      </li>
      <li>更新方法
 <br />对于感知机模型 $f(x)=sign(w*x+b)$
        <ol>
          <li>选取初值 $w_0, b_0$</li>
          <li>在训练集选择数据 $(x_i, y_i)$</li>
        </ol>
      </li>
    </ol>
  </li>
</ol>

  </section>
</article>

<section class="read-more">
   
   
   <div class="read-more-item">
       <span class="read-more-item-dim">最近的文章</span>
       <h2 class="post-list__post-title post-title"><a href="/2018/11/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/" title="link to 梯度下降法">梯度下降法</a></h2>
       <p class="excerpt">#&hellip;</p>
       <div class="post-list__meta"><time datetime="2018-11-26 00:00:00 +0800" class="post-list__meta--date date">2018-11-26</time> &#8226; <span class="post-list__meta--tags tags"></span><a class="btn-border-small" href=/2018/11/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/>继续阅读</a></div>
   </div>
   
   
   
   
   <div class="read-more-item">
       <span class="read-more-item-dim">更早的文章</span>
       <h2 class="post-list__post-title post-title"><a href="/2018/09/%E5%9B%9E%E5%BD%92%E5%92%8C%E5%88%86%E7%B1%BB/" title="link to 回归和分类">回归和分类</a></h2>
       <p class="excerpt">回归  区别                  从输出值、目的和评价指标来区分                                            区别              回归              分类                                                          输出              连续值              离散数据                                   ...&hellip;</p>
       <div class="post-list__meta"><time datetime="2018-09-28 00:00:00 +0800" class="post-list__meta--date date">2018-09-28</time> &#8226; <span class="post-list__meta--tags tags"></span><a class="btn-border-small" href=/2018/09/%E5%9B%9E%E5%BD%92%E5%92%8C%E5%88%86%E7%B1%BB/>继续阅读</a></div>
   </div>
   
</section>

<section class="post-comments">
  
    <div id="disqus_thread"></div>
    <script>
    
    var disqus_config = function () {
        this.page.url = "/2018/11/perception/";
        this.page.identifier = "/2018/11/perception/";
    };

    var disqus_shortname = 'vno-jekyll';
    
    (function() { // DON'T EDIT BELOW THIS LINE
        var d = document, s = d.createElement('script');
        s.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();
    </script>
    <noscript>要查看<a href="http://disqus.com/?ref_noscript"> Disqus </a>评论，请启用 JavaScript</noscript>
    
  
  
  
  
</section>


            <section class="footer">
    <footer>
    	<span class="footer__copyright">本站点采用<a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">知识共享 署名-非商业性使用-相同方式共享 4.0 国际 许可协议</a></span>
        <span class="footer__copyright">由 <a href="https://jekyllrb.com">Jekyll</a> 于 2018-11-26 生成，感谢 <a href="https://www.digitalocean.com/?refcode=30ed2d146762">Digital Ocean</a> 为本站提供稳定的 VPS 服务</span>
        <span class="footer__copyright">本站采用 <a href="https://github.com/onevcat/vno-jekyll">Vno - Jekyll</a> 作为主题，您可以在 GitHub 找到<a href="https://github.com/onevcat/OneV-s-Den">本站源码</a> - &copy; 2018</span>
    </footer>
</section>

        </div>
    </div>
    
    <script type="text/javascript" src="//code.jquery.com/jquery-1.11.3.min.js"></script>

<script type="text/javascript" src="/js/main.js"></script>



    
  </body>

</html>
