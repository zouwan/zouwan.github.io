<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>噢!乖</title>
    <description></description>
    <link>/</link>
    <atom:link href="/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Mon, 26 Nov 2018 21:01:28 +0800</pubDate>
    <lastBuildDate>Mon, 26 Nov 2018 21:01:28 +0800</lastBuildDate>
    <generator>Jekyll v3.7.4</generator>
    
      <item>
        <title>梯度下降法</title>
        <description>&lt;h2 id=&quot;梯度下降法&quot;&gt;梯度下降法&lt;/h2&gt;
</description>
        <pubDate>Mon, 26 Nov 2018 00:00:00 +0800</pubDate>
        <link>/2018/11/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/</link>
        <guid isPermaLink="true">/2018/11/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/</guid>
        
        
      </item>
    
      <item>
        <title>Perception</title>
        <description>&lt;head&gt;
    &lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
    &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    &lt;/script&gt;
&lt;/head&gt;
&lt;h3 id=&quot;1-什么是感知机perception&quot;&gt;1. 什么是感知机(Perception)？&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;b&gt;感知机&lt;/b&gt;(perceptron)是二类分类的线性分类模型,其输入为实例的特征向量,输出为实例的类别,取+1和-1二值.
    &lt;ul&gt;
      &lt;li&gt;感知机对应于输入空间中将实例划分为正负两类的分离超平面，属于 &lt;b&gt;判别模型&lt;/b&gt;。&lt;/li&gt;
      &lt;li&gt;感知机学习旨在求出将训练数据进行线性划分的分离超平面，为此导入了基于误分类的损失函数，利用梯度下降法对损失函数进行极小化，求得感知机模型。&lt;/li&gt;
      &lt;li&gt;感知机学习算法具有简单而易于实现的优点，分为原始形式和对偶形式。&lt;/li&gt;
      &lt;li&gt;感知机是神经网络与支持向量机的基础。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;2-感知机模型原理&quot;&gt;2. 感知机模型原理&lt;/h3&gt;
&lt;p&gt;感知机满足输入空间到输出空间的如下函数称为感知机：
    &lt;script type=&quot;math/tex&quot;&gt;f(x)=sign(wx+b)&lt;/script&gt; 其中 w 称为权值(weight，b 称为偏置(bias), sign为激活函数
    &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
sign(x)=\begin{cases} +1,x&gt;0 \\ -1,x&lt;0 \\ \end{cases} %]]&gt;&lt;/script&gt;
   sign(x) 将大于0的分为1， 小于0的分为-1.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;感知机相比逻辑回归主要区别是 &lt;b&gt;激活函数不同&lt;/b&gt;
    &lt;ol&gt;
      &lt;li&gt;感知机激活函数是 sign(x),又称单位阶跃函数&lt;/li&gt;
      &lt;li&gt;逻辑回归（logistic regression)激活函数是 sigmoid&lt;/li&gt;
      &lt;li&gt;sigmoid一般取阈值0.5， 大于0.5的分为1， 小于0.1的分为0&lt;/li&gt;
      &lt;li&gt;逻辑回归也被看做是一种概率估计， 详见逻辑回归(todo)&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;工作原理
    &lt;ol&gt;
      &lt;li&gt;给每一个属性一个权重，对属性和权重乘积求和得到的结果和一个阈值进行比较，可以输出一个二分类结果。比如判定是否能够给张三放贷。&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;感知机的几何解释:
    &lt;ol&gt;
      &lt;li&gt;感知机可以用线性方程表示；
 &lt;script type=&quot;math/tex&quot;&gt;w \cdot x+b=0&lt;/script&gt;&lt;/li&gt;
      &lt;li&gt;几何意义如下图:
 对应于特征空间Rn中的一个超平面S，其中w是超平面的法向量，b是截距。这个超平面将特征空间划分为两个部分，位于两部分的点分别被分为正、负两类。因此，超平面S称为分离超平面（separating hyperplanes）
 &lt;img src=&quot;https://pic2.zhimg.com/80/v2-7bd3d267a3d50a511b4b51aace348301_hd.png&quot; alt=&quot;image&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;啥？超平面 （黑脸问号)
        &lt;ol&gt;
          &lt;li&gt;超平面在二维空间里就是直线，方程是a&lt;em&gt;x+b&lt;/em&gt;y+c=0&lt;/li&gt;
          &lt;li&gt;超平面在三维空间里就是平面，方程是a&lt;em&gt;x+b&lt;/em&gt;y+c*z+d=0&lt;/li&gt;
          &lt;li&gt;在n维空间里推广就是就是a&lt;em&gt;x+b&lt;/em&gt;y+c*z+……..+k=0
(a,b,c…)对应向量w, 是由平面确定的数，
   （x,y,z..)是平面上任一点的坐标，对应方程的向量x&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;假定 $wx+b=0$是一个超平面，令$g(x)=wx+b$,即超平面上的所有点都满足$g(x)=0$. 对于超平面的一侧点满足 $g(x)&amp;gt;0$, 另一侧满足 $g(x)&amp;lt;0$.&lt;/li&gt;
      &lt;li&gt;对于不在超平面上的点x到超平面的距离是 $ r=\frac {g(x)}{\left|w \right|}, w为L2范数$
 证明：见下图，其中 O 为原点， $X_p$为超平面上的点， X 是超平面外的点， w 为超平面法向量
&lt;img src=&quot;https://img-blog.csdn.net/20171015173343056?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSmFzdGVyX3dpc2RvbQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center&quot; alt=&quot;image&quot; /&gt;&lt;/li&gt;
    &lt;/ol&gt;

    &lt;p&gt;(1) $X=X_p+r * \frac{w}{ \left|w\right|}$&lt;/p&gt;
    &lt;blockquote&gt;
      &lt;p&gt;向量基本运算法则：$ OX = OX_p + X_pX$; w为法向量，所以 $\frac{w}{\left|w\right|}$ 是垂直于超平面的单位向量&lt;/p&gt;
    &lt;/blockquote&gt;

    &lt;p&gt;(2) $g(x) = w^T(x_p+r\frac{w}{\left|w\right|})+w_0 = (w^T * x_p + w_0) + r * \frac{w^T * w}{\left|w\right|} = r * \left| w \right|$&lt;/p&gt;
    &lt;blockquote&gt;
      &lt;p&gt;等式（1）带入等式 $g(x)=wx+b$, 由于$X_p$在超平面， 所以 $g(X_p) = w^T * x_p + w_0 = 0$&lt;/p&gt;
    &lt;/blockquote&gt;

    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; 结论一得证
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;3-感知机学习目标&quot;&gt;3. 感知机学习目标&lt;/h3&gt;
&lt;p&gt;  学习参数w与b，确定了w与b，图上的直线（高维空间下为超平面）也就确定了; 感知机的学习目标是找到损失函数,转化为最优化问题&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;思路一：误分类点数目
    &lt;ol&gt;
      &lt;li&gt;无法函数化表示&lt;/li&gt;
      &lt;li&gt;w, b 不是连续可导&lt;/li&gt;
      &lt;li&gt;由于上述1，2两点，思路一不可行，考虑思路二&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;思路二：误分类点距离超平面距离，总距离越小越好
 &lt;script type=&quot;math/tex&quot;&gt;\frac{1}{\| w\|} | w \cdot x_0 + b |&lt;/script&gt;
    &lt;ol&gt;
      &lt;li&gt;当数据点被误分类(+1吧被误判为-1), 则 $(w \cdot x_0 + b) &amp;lt; 0 $, 此时预测值 $y_i$=+1, 满足 &lt;script type=&quot;math/tex&quot;&gt;-y_i(w \cdot x_0 + b) &gt; 0&lt;/script&gt;&lt;/li&gt;
      &lt;li&gt;当数据点被误分类(-1吧被误判为+1), 则 $(w \cdot x_0 + b) &amp;gt; 0 $, 此时预测值 $y_i$=-1, 满足 &lt;script type=&quot;math/tex&quot;&gt;y_i(w \cdot x_0 + b) &gt; 0&lt;/script&gt;&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;损失函数确定
    &lt;ol&gt;
      &lt;li&gt;
        &lt;p&gt;上式不考虑$|w|$, 去掉后面的绝对值，得到损失函数:
 &lt;script type=&quot;math/tex&quot;&gt;J(w,b) = -\sum_{x_i\in M}{y_i(w \cdot x_i + b)}，M为误分类点集&lt;/script&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;感知机不关心实际超平面距离每个点的距离（定量，误分类点的距离），只关心最终分类是否正确（定性，误分类点的个数）&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;损失函数推导：
    &lt;ol&gt;
      &lt;li&gt;把正例预测为负例： $w*x_i+b&amp;gt;0时， y_i=-1$&lt;/li&gt;
      &lt;li&gt;把负例预测为正例： $w*x_i+b&amp;lt;0时， y_i=1$&lt;/li&gt;
      &lt;li&gt;可以得到误分类点到超平面总的距离是（注意， 只考虑不在超平面的点）
 &lt;script type=&quot;math/tex&quot;&gt;-\frac{1}{\left\|w\right\|}\sum_{x_i \in M}{y_i*(w*x_i+b)}&lt;/script&gt; 
 其中$\left|w\right|$可以忽略（原因1：$frac{1}{\left|w\right|}$ 不影响正负的判断，即不影响学习算法的中间过程我们；原因2：感知机训练终止条件是所有样本正确分类, 即不存在误分类点，因此$frac{1}{\left|w\right|}$ 对最终结果无影响），得到损失函数：
 &lt;script type=&quot;math/tex&quot;&gt;-\sum_{x_i \in M}{y_i*(w*x_i+b)}&lt;/script&gt;&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;更新策略（梯度下降）
    &lt;ol&gt;
      &lt;li&gt;对损失函数计算梯度（梯度下降）
&lt;script type=&quot;math/tex&quot;&gt;\left\{ \begin{array}{lr} \nabla_{w}{L(w,b)} = -\sum_{x \in M}{y_i * x_i} \\ \nabla_{b}{L(w,b)} = -\sum_{x \in M}{y_i} \end{array} \right.&lt;/script&gt;&lt;br /&gt;
这里使用随机梯度（随机选取误分类点$(x_i, y_i)$对 w, b 进行更新， 更新公式如下：
&lt;script type=&quot;math/tex&quot;&gt;\left\{ \begin{array}{lr} w \leftarrow w+\eta y_i*x_i \\ b \leftarrow b+\eta y_i  \end{array} \right.&lt;/script&gt;&lt;/li&gt;
      &lt;li&gt;如何理解上面更新公式？(from &lt;a href=&quot;https://www.zhihu.com/question/57747902&quot;&gt;知乎&lt;/a&gt;)
        &lt;ol&gt;
          &lt;li&gt;梯度更新公式确实不是推导而是创造出来的，所以只能从概念上去理解&lt;/li&gt;
          &lt;li&gt;设想下有个函数，你的目标是：找到一个参数$\theta$ 使得它的值$Y$最小。但它很复杂，你无法找到这个参数的解析解，所以你希望通过梯度下降法去猜这个参数&lt;/li&gt;
          &lt;li&gt;问题是怎么猜？
 对于多数有连续性的函数来说，显然不可能把每个$\theta$都试一遍。所以只能先随机取一个$\theta$，然后看看怎么调整它最有可能使得$\theta$变小。
 把这个过程重复n遍，自然最后得到的损失函数L的估值会越来越小&lt;/li&gt;
          &lt;li&gt;调整策略（参考文章:&lt;a href=&quot;https://zouwan.github.io/2018/11/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/&quot;&gt;梯度下降法&lt;/a&gt;）
 基于当前我们拥有的那个参数$\theta$，所以有了：&lt;script type=&quot;math/tex&quot;&gt;\theta_{t+1}=\theta_t + \Delta&lt;/script&gt;那现在问题是每次更新的时候这个$\theta$应该取什么值？
 我们知道关于某变量的（偏）导数的概念是指当（仅仅）该变量往 &lt;b&gt;正向&lt;/b&gt; 的变化量趋向于0时的其函数值变化量的极限。若求Y关于$\theta_t$的导数，得到一个值比如+5, 那就说明若现在我们把$\theta_t$往负向移动，损失函数 $J(w,b）$的值会变小，但不一定是正好-5。同理若现在导数是-5，那么把$\theta$往 &lt;b&gt;负向&lt;/b&gt;移动，损失函数$J(w,b)$值会变小。&lt;br /&gt;
 不管导数值$\theta$是正的还是负的(正负即导数的方向), 对于$J(w,b)$来说，$-\theta$的最终方向（即最终的正负号，决定是增(+)还是减(-))一定是能将$J(w,b)$值变小的方向（除非导数为0）。所以有了:
 &lt;script type=&quot;math/tex&quot;&gt;\theta_{t+1}=\theta_t + (-\Delta)&lt;/script&gt;但是说到底，$\Delta$的绝对值只是个关于Y的变化率，本质上和$\theta_t$没关系。所以为了抹去$\Delta$在幅度上对$\theta_t$的影响，需要一个学习率来控制：$\alpha$。所以有了：$\alpha \in \left(0,1\right]$
 &lt;script type=&quot;math/tex&quot;&gt;\theta_{t+1}=\theta_t + (-\alpha \Delta) =\theta_t -\alpha \Delta&lt;/script&gt;就是有多少个参数，就有多少个不同的$\Delta$。
 &lt;br /&gt;现在分析在梯度下降法中最常听到的一句话：“梯度下降法就是朝着梯度的反方向迭代地调整参数直到收敛。” 这里的梯度就是 $\Delta$ ,而梯度的反方向就是 $-\Delta$ 的符号方向—梯度实际上是个向量。所以这个角度来说，即使我们只有一个参数需要调整，也可以认为它是个一维的向量。 
 &amp;lt;br&lt;b&gt;整个过程你可以想象自己站在一个山坡上，准备走到山脚下（最小值的地方），于是很自然地你会考虑朝着哪个方向走，方向由$\Delta$的方向给出，而至于一次走多远，由$|\alpha\Delta|$来控制。这种方式相信你应该能理解其只能找到局部最小值，而不是全局的。&lt;/b&gt;&lt;/li&gt;
          &lt;li&gt;换个角度看，想象损失函数是$y=x^2$的抛物线形式，其梯度为 $y’=2x$
            &lt;ol&gt;
              &lt;li&gt;纵坐标y表示损失函数，横坐标x表示参数 $\theta$&lt;/li&gt;
              &lt;li&gt;$\theta$在最低点左边时，导数小于0，只有$\theta$往右边（最低点方向）走，损失函数才会变小，即$\theta$要往导数的负方向变化
                &lt;ol&gt;
                  &lt;li&gt;当 x=-2; y=-4, 此时往负向损失函数$y$增大，往正向走损失函数$y$减少&lt;/li&gt;
                &lt;/ol&gt;
              &lt;/li&gt;
              &lt;li&gt;$\theta$在最低点右边时，导数大于0，只有$\theta$往左边（最低点方向）走，损失函数才会变小，即$\theta$要往导数的负方向变化
 1.当 x=+2; y=+4, 此时往正向损失函数$y$增大，往负向走损失函数$y$减少          &lt;br /&gt;
 &lt;img src=&quot;/assets/images//15432151219295.jpg&quot; alt=&quot;-w450&quot; /&gt;&lt;/li&gt;
            &lt;/ol&gt;
          &lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;更新方法
 &lt;br /&gt;对于感知机模型 $f(x)=sign(w*x+b)$
        &lt;ol&gt;
          &lt;li&gt;选取初值 $w_0, b_0$&lt;/li&gt;
          &lt;li&gt;在训练集选择数据 $(x_i, y_i)$
            &lt;ol&gt;
              &lt;li&gt;任意抽取数据点，当所有数据点没有误分类时结束算法，否则进入3.&lt;/li&gt;
            &lt;/ol&gt;
          &lt;/li&gt;
          &lt;li&gt;此时有 $y_i(w&lt;em&gt;x_i+b)&amp;lt;0$ ， 更新参数:
 $\left{ \begin{array}{lr} w \leftarrow w+\eta y_i&lt;/em&gt;x_i \ b \leftarrow b+\eta y_i  \end{array} \right. $&lt;/li&gt;
          &lt;li&gt;其中$\eta$ 学习率一般为0-1之间&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;来段代码解析
        &lt;ol&gt;
          &lt;li&gt;例题来自李航《统计学习方法》的感知机部分
 &lt;img src=&quot;https://pic1.zhimg.com/80/v2-0575c3efd5d8e1322aaf1d6ef18808ac_hd.jpg&quot; alt=&quot;&quot; /&gt;&lt;/li&gt;
          &lt;li&gt;例题解析
            &lt;ol&gt;
              &lt;li&gt;构建优化函数： $min_{w,b}J(w,b) = -\sum_{x_i \in M}y_i(w \cdot x + b)$&lt;/li&gt;
              &lt;li&gt;求解w, b, 为了计算方便，设 $\eta=0.1$ （$\eta$为学习率，建议设置 0-1之间）&lt;/li&gt;
            &lt;/ol&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;步骤
     取初值: w=[0 0] b=1
     0:w=[0 0] b=1x=[3 3] 预测正确,跳过
     0:w=[0 0] b=1x=[4 3] 预测正确,跳过
     0:w=[0 0] b=1x=[1 1] 预测错误,更新w,b
     i=0 w=[-1 -1] b=0&lt;/p&gt;

            &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; 1:w=[-1 -1] b=0x=[3 3] 预测错误,更新w,b
 i=1 w=[2 2] b=1
            
 2:w=[2 2] b=1x=[3 3] 预测正确,跳过
 2:w=[2 2] b=1x=[4 3] 预测正确,跳过
 2:w=[2 2] b=1x=[1 1] 预测错误,更新w,b
 i=2 w=[1 1] b=0
            
 3:w=[1 1] b=0x=[3 3] 预测正确,跳过
 3:w=[1 1] b=0x=[4 3] 预测正确,跳过
 3:w=[1 1] b=0x=[1 1] 预测错误,更新w,b
 i=3 w=[0 0] b=-1
            
 4:w=[0 0] b=-1x=[3 3] 预测错误,更新w,b
 i=4 w=[3 3] b=0
            
 5:w=[3 3] b=0x=[3 3] 预测正确,跳过
 5:w=[3 3] b=0x=[4 3] 预测正确,跳过
 5:w=[3 3] b=0x=[1 1] 预测错误,更新w,b
 i=5 w=[2 2] b=-1
            
 6:w=[2 2] b=-1x=[3 3] 预测正确,跳过
 6:w=[2 2] b=-1x=[4 3] 预测正确,跳过
 6:w=[2 2] b=-1x=[1 1] 预测错误,更新w,b
 i=6 w=[1 1] b=-2
            
 7:w=[1 1] b=-2x=[3 3] 预测正确,跳过
 7:w=[1 1] b=-2x=[4 3] 预测正确,跳过
 7:w=[1 1] b=-2x=[1 1] 预测错误,更新w,b
 i=7 w=[0 0] b=-3
            
 8:w=[0 0] b=-3x=[3 3] 预测错误,更新w,b
 i=8 w=[3 3] b=-2
            
 9:w=[3 3] b=-2x=[3 3] 预测正确,跳过
 9:w=[3 3] b=-2x=[4 3] 预测正确,跳过
 9:w=[3 3] b=-2x=[1 1] 预测错误,更新w,b
 i=9 w=[2 2] b=-3
            
 10:w=[2 2] b=-3x=[3 3] 预测正确,跳过
 10:w=[2 2] b=-3x=[4 3] 预测正确,跳过
 10:w=[2 2] b=-3x=[1 1] 预测错误,更新w,b
 i=10 w=[1 1] b=-4
            
 11:w=[1 1] b=-4x=[3 3] 预测正确,跳过
 11:w=[1 1] b=-4x=[4 3] 预测正确,跳过
 11:w=[1 1] b=-4x=[1 1] 预测正确,跳过
 w=[1 1] b= 第11 次更新时得到解
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;            &lt;/div&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;代码&lt;/p&gt;

            &lt;p&gt;```python
     #李航《统计学方法》p29 例2.1, 正例：x1=(3,3), x2=(4,3),负例：x3=(1,1)&lt;/p&gt;

            &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; import numpy as np
 import matplotlib.pyplot as plt
 # x取值，样本数据，对应的是一个二维向量
 p_x = np.array([[3, 3], [4, 3], [1, 1]])
 # y取值，样本数据对应的正负标签，-1=负样本， +1=正样本
 y = np.array([1, 1, -1])
 # 输出样本数据，其中正样本为红色，负样本为蓝色
 plt.figure()
 for i in range(len(p_x)):
     if y[i] == 1:
         plt.plot(p_x[i][0], p_x[i][1], 'ro')
     else:
         plt.plot(p_x[i][0], p_x[i][1], 'bo')
 w = np.array([0, 0])
 b = 1
 delta = 1
             
 print(&quot;取初值: w=&quot; + str(w) + &quot; b=&quot; + str(b))
 for i in range(100):
     choice = -1
     for j in range(len(p_x)):
         y_pred = np.sign(np.dot(w, p_x[j]) + b)
         if y[j] != y_pred:
             print(str(i)+&quot;:w=&quot;+str(w)+&quot; b=&quot;+str(b)+&quot;x=&quot;+str(p_x[j])+&quot; 预测错误,更新w,b&quot;)
             choice = j
             break
         else:
             print(str(i)+&quot;:w=&quot;+str(w)+&quot; b=&quot;+str(b)+&quot;x=&quot;+str(p_x[j])+&quot; 预测正确,跳过&quot;)
            
     if choice == -1:
         print(&quot;w=&quot; + str(w) + &quot; b=&quot; + &quot; 第&quot; + str(i) + &quot; 次更新时得到解&quot;)
         break
     w = w + delta * y[choice]*p_x[choice]
     b = b + delta * y[choice]
     print(&quot;i=&quot; + str(i) + &quot; w=&quot; + str(w) +&quot; b=&quot; + str(b) + &quot;\n&quot;)
             
 line_x = [0, 10]
 line_y = [0, 0]
             
 for i in range(len(line_x)):
     line_y[i] = (-w[0] * line_x[i]- b)/w[1]
             
 plt.plot(line_x, line_y)
        
 ```
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;            &lt;/div&gt;
          &lt;/li&gt;
          &lt;li&gt;输出
&lt;img src=&quot;/assets/images//15432216556174.jpg&quot; alt=&quot;&quot; /&gt;&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Sun, 25 Nov 2018 00:00:00 +0800</pubDate>
        <link>/2018/11/perception/</link>
        <guid isPermaLink="true">/2018/11/perception/</guid>
        
        
      </item>
    
      <item>
        <title>回归和分类</title>
        <description>&lt;h3 id=&quot;回归&quot;&gt;回归&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;区别
    &lt;ol&gt;
      &lt;li&gt;
        &lt;p&gt;从输出值、目的和评价指标来区分&lt;/p&gt;

        &lt;table&gt;
          &lt;thead&gt;
            &lt;tr&gt;
              &lt;th style=&quot;text-align: center&quot;&gt;区别&lt;/th&gt;
              &lt;th style=&quot;text-align: center&quot;&gt;回归&lt;/th&gt;
              &lt;th style=&quot;text-align: center&quot;&gt;分类&lt;/th&gt;
            &lt;/tr&gt;
          &lt;/thead&gt;
          &lt;tbody&gt;
            &lt;tr&gt;
              &lt;td style=&quot;text-align: center&quot;&gt;输出&lt;/td&gt;
              &lt;td style=&quot;text-align: center&quot;&gt;连续值&lt;/td&gt;
              &lt;td style=&quot;text-align: center&quot;&gt;离散数据&lt;/td&gt;
            &lt;/tr&gt;
            &lt;tr&gt;
              &lt;td style=&quot;text-align: center&quot;&gt;目的&lt;/td&gt;
              &lt;td style=&quot;text-align: center&quot;&gt;寻找最优拟合&lt;/td&gt;
              &lt;td style=&quot;text-align: center&quot;&gt;寻找决策边界&lt;/td&gt;
            &lt;/tr&gt;
            &lt;tr&gt;
              &lt;td style=&quot;text-align: center&quot;&gt;评价方法&lt;/td&gt;
              &lt;td style=&quot;text-align: center&quot;&gt;SSE(sum of square errors)、拟合优度&lt;/td&gt;
              &lt;td style=&quot;text-align: center&quot;&gt;精度、混淆矩阵&lt;/td&gt;
            &lt;/tr&gt;
          &lt;/tbody&gt;
        &lt;/table&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;从应用场景看&lt;/p&gt;
        &lt;ol&gt;
          &lt;li&gt;回归问题的应用场景&lt;br /&gt;
 回归问题通常是用来预测一个值，如预测房价、未来的天气情况等等，例如一个产品的实际价格为500元，通过回归分析预测值为499元，我们认为这是一个比较好的回归分析。一个比较常见的回归算法是线性回归算法（LR）。另外，回归分析用在神经网络上，其最上层是不需要加上softmax函数的，而是直接对前一层累加即可。回归是对真实值的一种逼近预测。&lt;/li&gt;
          &lt;li&gt;分类问题的应用场景
 分类问题是用于将事物打上一个标签，通常结果为离散值。例如判断一幅图片上的动物是一只猫还是一只狗，分类通常是建立在回归之上，分类的最后一层通常要使用softmax函数进行判断其所属类别。分类并没有逼近的概念，最终正确结果只有一个，错误的就是错误的，不会有相近的概念。最常见的分类方法是逻辑回归，或者叫逻辑分类。&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;logistic 回归是分类
    &lt;ol&gt;
      &lt;li&gt;logistic回归只是用到了回归算法，但是其输出的结果是决策边界，是不连续的。&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;逻辑回归5要素
    &lt;ol&gt;
      &lt;li&gt;假设
        &lt;ol&gt;
          &lt;li&gt;数据服从伯努利分布,模型可以描述为
  &lt;script type=&quot;math/tex&quot;&gt;h_\theta(x;\theta) = p&lt;/script&gt;&lt;/li&gt;
          &lt;li&gt;假设正样本的概率是
  &lt;script type=&quot;math/tex&quot;&gt;p = \frac{1}{(1 + e^{-\theta^{T}x})}&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;h_\theta(x;\theta) = \frac{1}{(1 + e^{-\theta^{T}x})}&lt;/script&gt;&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;损失函数
        &lt;ol&gt;
          &lt;li&gt;极大似然函数
 &lt;script type=&quot;math/tex&quot;&gt;\prod_{1}^{m}h_{\theta}(x^i;\theta)^yi * (1 - h_\theta(x^i;\theta))^{1-y^i}&lt;/script&gt;&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;求解方法
        &lt;ol&gt;
          &lt;li&gt;梯度下降求解参数
            &lt;ol&gt;
              &lt;li&gt;批量梯度下降 batch gd&lt;/li&gt;
              &lt;li&gt;随机梯度下降 sgd&lt;/li&gt;
              &lt;li&gt;小批量梯度下降&lt;/li&gt;
            &lt;/ol&gt;
          &lt;/li&gt;
          &lt;li&gt;学习率选择
            &lt;ol&gt;
              &lt;li&gt;先大后小&lt;/li&gt;
              &lt;li&gt;更新频繁的参数
                &lt;ol&gt;
                  &lt;li&gt;选择较小学习率&lt;/li&gt;
                  &lt;li&gt;否则，选择较大学习率&lt;/li&gt;
                &lt;/ol&gt;
              &lt;/li&gt;
            &lt;/ol&gt;
          &lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;目的
        &lt;ol&gt;
          &lt;li&gt;数据二分类&lt;/li&gt;
          &lt;li&gt;提高准确率&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;如何分类
        &lt;ol&gt;
          &lt;li&gt;选定分类阈值&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Fri, 28 Sep 2018 00:00:00 +0800</pubDate>
        <link>/2018/09/%E5%9B%9E%E5%BD%92%E5%92%8C%E5%88%86%E7%B1%BB/</link>
        <guid isPermaLink="true">/2018/09/%E5%9B%9E%E5%BD%92%E5%92%8C%E5%88%86%E7%B1%BB/</guid>
        
        
      </item>
    
      <item>
        <title>基数估计算法</title>
        <description>&lt;p&gt;转自淘宝张洋的基数估计算法概览&lt;/p&gt;

&lt;h3 id=&quot;基数估计算法定义&quot;&gt;基数估计算法定义&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;基数是指一个集合中，不同的数的个数；基数统计是集合不同的数的个数
    &lt;ul&gt;
      &lt;li&gt;比如一个集合｛0, 1, 2, 2, 4, 5}，其基数是5&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;基数估计是估计一个集合中不同数的个数
    &lt;ul&gt;
      &lt;li&gt;用概率算法的思想，来用低空间和时间成本，以一个很低的误差度来估计数据的基数&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;原理&quot;&gt;原理&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;一个简单直观的基数估计方法
  假设你通过如下步骤生成了一个数据集：
    &lt;ol&gt;
      &lt;li&gt;随机生成n个服从均匀分布的数字&lt;/li&gt;
      &lt;li&gt;随便重复其中一些数字，重复的数字和重复次数都不确定&lt;/li&gt;
      &lt;li&gt;打乱这些数字的顺序，得到一个数据集&lt;/li&gt;
    &lt;/ol&gt;

    &lt;p&gt;问题：如何估计这个数据集中有多少不同的数字？&lt;/p&gt;
    &lt;ol&gt;
      &lt;li&gt;找出数据集中最小的数字&lt;/li&gt;
      &lt;li&gt;假如m是数值上限，x是找到的最小的数，则m/x是基数的一个估计&lt;/li&gt;
    &lt;/ol&gt;

    &lt;p&gt;例如，我们扫描一个包含0到1之间数字组成的数据集，其中最小的数是0.01，则一个比较合理的推断是数据集中大约有100个不同的元素，否则我们应该预期能找到一个更小的数。注意这个估计值和重复次数无关：就如最小值重复多少次都不改变最小值的数值&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;改进方案
  上面的简单基数估计存在精度不足的问题&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;优势&quot;&gt;优势&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;基数估计算法使用很少的资源给出数据集基数的一个良好估计&lt;/li&gt;
  &lt;li&gt;这个方法和数据本身的特征无关，而且可以高效的进行分布式并行计算&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;应用场景&quot;&gt;应用场景&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;应用场景
    &lt;ul&gt;
      &lt;li&gt;流量监控（多少不同IP访问过一个服务器）&lt;/li&gt;
      &lt;li&gt;数据库查询优化（例如我们是否需要排序和合并，或者是否需要构建哈希表）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;参考&quot;&gt;参考&lt;/h3&gt;
&lt;p&gt;http://blog.codinglabs.org/articles/cardinality-estimation.html
https://www.jianshu.com/p/a966e7d71666&lt;/p&gt;
</description>
        <pubDate>Thu, 27 Sep 2018 00:00:00 +0800</pubDate>
        <link>/2018/09/%E5%9F%BA%E6%95%B0%E4%BC%B0%E8%AE%A1%E7%AE%97%E6%B3%95/</link>
        <guid isPermaLink="true">/2018/09/%E5%9F%BA%E6%95%B0%E4%BC%B0%E8%AE%A1%E7%AE%97%E6%B3%95/</guid>
        
        
      </item>
    
      <item>
        <title>切单项目小结</title>
        <description>&lt;h3 id=&quot;背景&quot;&gt;背景&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;场站交易线下交易严重。带来了如下问题
    &lt;ol&gt;
      &lt;li&gt;平台收益受损；&lt;/li&gt;
      &lt;li&gt;乘客体验差;&lt;/li&gt;
      &lt;li&gt;容易引发线下超载等安全事故&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;场站切单率.预计至少超过20%
    &lt;ol&gt;
      &lt;li&gt;客服随机电话回访&lt;/li&gt;
      &lt;li&gt;场站订单司机行为分析&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;思路&quot;&gt;思路&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;根据司机历史统计行为进行建模&lt;/li&gt;
  &lt;li&gt;司机在刷订单列表时，根据模型（司机特征、乘客特征、订单特征）预测切单概率&lt;/li&gt;
  &lt;li&gt;对切单倾向严重的匹配做管控
    &lt;ol&gt;
      &lt;li&gt;最严重的直接过滤&lt;/li&gt;
      &lt;li&gt;其他司机按照其平均成交金额做分级管控&lt;/li&gt;
      &lt;li&gt;根据实时供需数据随时调整阈值（doing.&lt;/li&gt;
      &lt;li&gt;从安全角度应该直接清除，从增长角度需要区分场景
        &lt;ol&gt;
          &lt;li&gt;运力不足：适当放松阈值，优先满足乘客乘坐，同时平台有较低概率能够拿到提成&lt;/li&gt;
          &lt;li&gt;运力充足：适当收紧阈值，优先让平台上的优质司机(切单概率低、忠诚度高.接单&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;特征工程&quot;&gt;特征工程&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;数据清洗
    &lt;ol&gt;
      &lt;li&gt;数据质量
        &lt;ol&gt;
          &lt;li&gt;数据完整性–例如人的属性缺少性别、籍贯、年龄等
            &lt;ol&gt;
              &lt;li&gt;数据补全（剔除）&lt;/li&gt;
            &lt;/ol&gt;
          &lt;/li&gt;
          &lt;li&gt;数据唯一性–不同来源数据出现重复情况
            &lt;ol&gt;
              &lt;li&gt;去除重复记录&lt;/li&gt;
            &lt;/ol&gt;
          &lt;/li&gt;
          &lt;li&gt;数据权威性–同一个指标出现多个来源的数据，且数值不一样
            &lt;ol&gt;
              &lt;li&gt;选用最权威渠道数据&lt;/li&gt;
            &lt;/ol&gt;
          &lt;/li&gt;
          &lt;li&gt;数据合法性–获取的数据与尝试不符，如年龄大于200岁
            &lt;ol&gt;
              &lt;li&gt;设定判定规则（或者半人工处理）&lt;/li&gt;
            &lt;/ol&gt;
          &lt;/li&gt;
          &lt;li&gt;数据一致性–不同来源的不同指标，实际内涵是一样的（或统一指标内涵不一致）
            &lt;ol&gt;
              &lt;li&gt;建立数据体系&lt;/li&gt;
            &lt;/ol&gt;
          &lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;数据更适合挖掘
        &lt;ol&gt;
          &lt;li&gt;高纬度
            &lt;ol&gt;
              &lt;li&gt;降维（主成分分析、随机森林）&lt;/li&gt;
            &lt;/ol&gt;
          &lt;/li&gt;
          &lt;li&gt;维度低
            &lt;ol&gt;
              &lt;li&gt;抽象
                &lt;ol&gt;
                  &lt;li&gt;汇总（平均、加总、最大、最小）&lt;/li&gt;
                  &lt;li&gt;离散化、聚类、自定义分组&lt;/li&gt;
                &lt;/ol&gt;
              &lt;/li&gt;
            &lt;/ol&gt;
          &lt;/li&gt;
          &lt;li&gt;无关信息和字段冗余
            &lt;ol&gt;
              &lt;li&gt;剔除&lt;/li&gt;
            &lt;/ol&gt;
          &lt;/li&gt;
          &lt;li&gt;多指标数值、单位不同
            &lt;ol&gt;
              &lt;li&gt;归一化（最小-最大、零-均值、小数定标）&lt;/li&gt;
            &lt;/ol&gt;
          &lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;缺失值处理（https://www.zhihu.com/question/26639110）
    &lt;ol&gt;
      &lt;li&gt;删除。最简单最直接的方法，很多时候也是最有效的方法，这种做法的缺点是可能会导致信息丢失。
        &lt;ol&gt;
          &lt;li&gt;删除有缺失数据的样本&lt;/li&gt;
          &lt;li&gt;删除有过多缺失数据的特征&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;补全。
        &lt;ol&gt;
          &lt;li&gt;用规则或模型将缺失数据补全，这种做法的缺点是可能会引入噪声。平均数、中位数、众数、最大值、最小值、固定值、插值等等&lt;/li&gt;
          &lt;li&gt;建立一个模型来“预测”缺失的数据。（KNN, Matrix completion等方法）
            &lt;ol&gt;
              &lt;li&gt;根本缺陷：如果其他变量与缺失变量无关，则预测结果无意义；反之如果预测结果非常准确，则说明加入这个变量无意义；&lt;/li&gt;
              &lt;li&gt;一般情况下，介于二者之间&lt;/li&gt;
            &lt;/ol&gt;
          &lt;/li&gt;
          &lt;li&gt;引入虚拟变量(dummy variable.来表征是否有缺失，是否有补全。&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;忽略。
        &lt;ol&gt;
          &lt;li&gt;有一些模型，如随机森林，自身能够处理数据缺失的情况，在这种情况下不需要对缺失数据做任何的处理，这种做法的缺点是在模型的选择上有局限。&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;把变量映射到高维空间。
        &lt;ol&gt;
          &lt;li&gt;好处：保留了原始数据全部信息，不用考虑缺失值、不用考虑线性不可分之类问题&lt;/li&gt;
          &lt;li&gt;缺点：计算量太大，而且需要样本量足够大&lt;/li&gt;
          &lt;li&gt;举例：性别有男、女、缺失三种情况，映射为三个变量：是否男、是否女、是否缺失。连续型变量也可以如此处理。&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;缺失值较多
        &lt;ol&gt;
          &lt;li&gt;修改为  “XX字段有值”、”字段为空”&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;特征转化
    &lt;ol&gt;
      &lt;li&gt;&lt;strong&gt;对原始特征转化，把原来的非线性关系转化为线性关系&lt;/strong&gt;
        &lt;ol&gt;
          &lt;li&gt;LinearReg模型是线性模型，在非线性情况下效果不好&lt;/li&gt;
          &lt;li&gt;复杂模型(如Svm)对于高维特征有时间约束&lt;/li&gt;
          &lt;li&gt;CTR 预估目前最常用方法还是 LR&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;方法1:离散化
        &lt;ol&gt;
          &lt;li&gt;目标：转化后向量里每个元素保持比较好的线性关系&lt;/li&gt;
          &lt;li&gt;离散化方法
            &lt;ol&gt;
              &lt;li&gt;等距离离散&lt;/li&gt;
              &lt;li&gt;等样本点离散&lt;/li&gt;
              &lt;li&gt;画图观察趋势（趋势、拐点）&lt;/li&gt;
            &lt;/ol&gt;
          &lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;方法2: 函数变换
        &lt;ol&gt;
          &lt;li&gt;通过非线性函数变换得到新的特征加入模型训练（需要对新加入特征做归一化)&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;方法3: 决策树里散发（切单采用）
        &lt;ol&gt;
          &lt;li&gt;决策树可以理解为一堆的 if … else …&lt;/li&gt;
          &lt;li&gt;天生可以对连续特征进行分段&lt;/li&gt;
          &lt;li&gt;gmail 在对信件重要性排序使用了决策树离散化方法&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;方法4: 核方法
        &lt;ol&gt;
          &lt;li&gt;可以理解为特征函数变换的一种方式&lt;/li&gt;
          &lt;li&gt;把核函数看成相似度的话， 则变成 KNN 模型或者加权平均模型&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;特征工程方法论
    &lt;ol&gt;
      &lt;li&gt;避免简单粗暴的归一化或者标准化，往往风险大于收益&lt;/li&gt;
      &lt;li&gt;尝试随机森林或者其他集成学习树模型暴力处理
        &lt;ol&gt;
          &lt;li&gt;决策树原型的模型对特征变量取值范围不敏感&lt;/li&gt;
          &lt;li&gt;有类似良好特性的分类器还包括:
            &lt;ol&gt;
              &lt;li&gt;特定种类的深度网络&lt;/li&gt;
              &lt;li&gt;L1范数正则化后的线性模型&lt;/li&gt;
            &lt;/ol&gt;
          &lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;移除不必要数据，降低变量维度
 在对特征做各种维度变化和复杂处理前，可以去掉无用和低贡献度的变量，降低后续处理难度
        &lt;ol&gt;
          &lt;li&gt;移除单一取值变量&lt;/li&gt;
          &lt;li&gt;移除低方差变量
            &lt;ol&gt;
              &lt;li&gt;和单一取值类似，虽然取值不唯一，但整体变化很小。&lt;/li&gt;
              &lt;li&gt;可人为设定阈值来去除该类型变量&lt;/li&gt;
            &lt;/ol&gt;
          &lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;转化描述变量
在不假设分类器的前提下，必须对描述变量转化为数字类型变量，因为大部分算法无法直接处理描述变量
        &lt;ol&gt;
          &lt;li&gt;连续特征(continuous)
            &lt;ol&gt;
              &lt;li&gt;归一化（去中心、方差归一）&lt;/li&gt;
            &lt;/ol&gt;
          &lt;/li&gt;
          &lt;li&gt;无序变量(特征)（categorical)
            &lt;ol&gt;
              &lt;li&gt;独热编码(one-hot、One-of-k),比如 color 取值为:
                &lt;ol&gt;
                  &lt;li&gt;red   (1,0,0)&lt;/li&gt;
                  &lt;li&gt;green (0,1,0)&lt;/li&gt;
                  &lt;li&gt;blue  (0,0,1)&lt;/li&gt;
                &lt;/ol&gt;
              &lt;/li&gt;
            &lt;/ol&gt;
          &lt;/li&gt;
          &lt;li&gt;有序变量(特征)（ordinal)
            &lt;ol&gt;
              &lt;li&gt;类似one-hot,包含顺序特征,比如status取值:
                &lt;ol&gt;
                  &lt;li&gt;bad      (1,0,0)&lt;/li&gt;
                  &lt;li&gt;normal   (1,1,0)&lt;/li&gt;
                  &lt;li&gt;good     (1,1,1)&lt;/li&gt;
                &lt;/ol&gt;
              &lt;/li&gt;
            &lt;/ol&gt;
          &lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Mon, 24 Sep 2018 00:00:00 +0800</pubDate>
        <link>/2018/09/%E5%88%87%E5%8D%95%E9%A1%B9%E7%9B%AE%E5%B0%8F%E7%BB%93/</link>
        <guid isPermaLink="true">/2018/09/%E5%88%87%E5%8D%95%E9%A1%B9%E7%9B%AE%E5%B0%8F%E7%BB%93/</guid>
        
        
      </item>
    
      <item>
        <title>词向量</title>
        <description>&lt;h3 id=&quot;词向量相关知识整理&quot;&gt;词向量相关知识整理&lt;/h3&gt;
&lt;hr /&gt;
&lt;h4 id=&quot;背景&quot;&gt;背景&lt;/h4&gt;
</description>
        <pubDate>Tue, 18 Sep 2018 00:00:00 +0800</pubDate>
        <link>/2018/09/%E8%AF%8D%E5%90%91%E9%87%8F/</link>
        <guid isPermaLink="true">/2018/09/%E8%AF%8D%E5%90%91%E9%87%8F/</guid>
        
        
      </item>
    
      <item>
        <title>从零搭建酒店类垂直搜索系统</title>
        <description>&lt;h3 id=&quot;前言&quot;&gt;前言&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;13年有机会帮前东家重构了搜索系统，一直忙忙碌碌，也没回头梳理一下，这里希望能把自己的实践和思考记录下来&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;背景&quot;&gt;背景&lt;/h3&gt;

&lt;h3 id=&quot;选型&quot;&gt;选型&lt;/h3&gt;

&lt;h3 id=&quot;架构&quot;&gt;架构&lt;/h3&gt;

&lt;h3 id=&quot;策略&quot;&gt;策略&lt;/h3&gt;
</description>
        <pubDate>Tue, 18 Sep 2018 00:00:00 +0800</pubDate>
        <link>/2018/09/%E4%BB%8E%E9%9B%B6%E6%90%AD%E5%BB%BA%E9%85%92%E5%BA%97%E7%B1%BB%E5%9E%82%E7%9B%B4%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/</link>
        <guid isPermaLink="true">/2018/09/%E4%BB%8E%E9%9B%B6%E6%90%AD%E5%BB%BA%E9%85%92%E5%BA%97%E7%B1%BB%E5%9E%82%E7%9B%B4%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/</guid>
        
        
      </item>
    
      <item>
        <title>Elasticsearch学习笔记</title>
        <description>&lt;h3 id=&quot;背景&quot;&gt;背景&lt;/h3&gt;
&lt;p&gt;1) 场站交易线下交易严重。带来了如下问题
    1) 平台收益受损； 
    2) 乘客体验差;
    3) 容易引发线下超载等安全事故
2) 场站切单率预计至少超过20%
    1) 客服随机电话回访
    2) 场站订单司机行为分析&lt;/p&gt;

&lt;h3 id=&quot;思路&quot;&gt;思路&lt;/h3&gt;
&lt;p&gt;1) 根据司机历史统计行为进行建模
2) 司机在刷订单列表时，根据模型（司机特征、乘客特征、订单特征）预测切单概率
3) 对切单倾向严重的匹配做管控
    1) 最严重的直接过滤
    2) 其他司机按照其平均成交金额做分级管控
    3) 根据实时供需数据随时调整阈值（doing)
    4) 从安全角度应该直接清除，从增长角度需要区分场景
        1) 运力不足：适当放松阈值，优先满足乘客乘坐，同时平台有较低概率能够拿到提成
        2) 运力充足：适当收紧阈值，优先让平台上的优质司机(切单概率低、忠诚度高)接单
4) 根据历史订单筛选出正负样本
    1)&lt;/p&gt;

&lt;h3 id=&quot;特征工程&quot;&gt;特征工程&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;缺失值处理（https://www.zhihu.com/question/26639110）
    &lt;ol&gt;
      &lt;li&gt;删除。最简单最直接的方法，很多时候也是最有效的方法，这种做法的缺点是可能会导致信息丢失。
        &lt;ol&gt;
          &lt;li&gt;删除有缺失数据的样本&lt;/li&gt;
          &lt;li&gt;删除有过多缺失数据的特征&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;补全。
        &lt;ol&gt;
          &lt;li&gt;用规则或模型将缺失数据补全，这种做法的缺点是可能会引入噪声。平均数、中位数、众数、最大值、最小值、固定值、插值等等&lt;/li&gt;
          &lt;li&gt;建立一个模型来“预测”缺失的数据。（KNN, Matrix completion等方法）
            &lt;ol&gt;
              &lt;li&gt;根本缺陷：如果其他变量与缺失变量无关，则预测结果无意义；反之如果预测结果非常准确，则说明加入这个变量无意义；&lt;/li&gt;
              &lt;li&gt;一般情况下，介于二者之间&lt;/li&gt;
            &lt;/ol&gt;
          &lt;/li&gt;
          &lt;li&gt;引入虚拟变量(dummy variable)来表征是否有缺失，是否有补全。&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;忽略。
        &lt;ol&gt;
          &lt;li&gt;有一些模型，如随机森林，自身能够处理数据缺失的情况，在这种情况下不需要对缺失数据做任何的处理，这种做法的缺点是在模型的选择上有局限。&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;把变量映射到高维空间。
        &lt;ol&gt;
          &lt;li&gt;好处：保留了原始数据全部信息，不用考虑缺失值、不用考虑线性不可分之类问题&lt;/li&gt;
          &lt;li&gt;缺点：计算量太大，而且需要样本量足够大&lt;/li&gt;
          &lt;li&gt;举例：性别有男、女、缺失三种情况，映射为三个变量：是否男、是否女、是否缺失。连续型变量也可以如此处理。&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;缺失值较多
        &lt;ol&gt;
          &lt;li&gt;
            &lt;table&gt;
              &lt;tbody&gt;
                &lt;tr&gt;
                  &lt;td&gt;修改为  字段有值&lt;/td&gt;
                  &lt;td&gt;字段为空&lt;/td&gt;
                &lt;/tr&gt;
              &lt;/tbody&gt;
            &lt;/table&gt;
          &lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Tue, 18 Sep 2018 00:00:00 +0800</pubDate>
        <link>/2018/09/ElasticSearch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</link>
        <guid isPermaLink="true">/2018/09/ElasticSearch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</guid>
        
        
      </item>
    
  </channel>
</rss>
