<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>噢!乖</title>
    <description></description>
    <link>/</link>
    <atom:link href="/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Thu, 06 Dec 2018 18:31:44 +0800</pubDate>
    <lastBuildDate>Thu, 06 Dec 2018 18:31:44 +0800</lastBuildDate>
    <generator>Jekyll v3.7.4</generator>
    
      <item>
        <title>Easy svm</title>
        <description>&lt;h1 id=&quot;浅入浅出svm&quot;&gt;浅入浅出SVM&lt;/h1&gt;
&lt;h2 id=&quot;question-list&quot;&gt;Question List&lt;/h2&gt;
&lt;p&gt;Q1. 什么是SVM,他能干什么
Q2. Svm在ML算法库中处于什么位置？&lt;/p&gt;
&lt;h2 id=&quot;svm基础知识&quot;&gt;SVM基础知识&lt;/h2&gt;
&lt;p&gt;SVM(support vector machine),中文也叫支持向量机。&lt;/p&gt;
</description>
        <pubDate>Thu, 06 Dec 2018 00:00:00 +0800</pubDate>
        <link>/2018/12/easy-svm/</link>
        <guid isPermaLink="true">/2018/12/easy-svm/</guid>
        
        
      </item>
    
      <item>
        <title>Svm</title>
        <description>&lt;h2 id=&quot;svm-支持向量机&quot;&gt;SVM-支持向量机&lt;/h2&gt;
&lt;h3 id=&quot;1-svm-的目标&quot;&gt;1. SVM 的目标&lt;/h3&gt;
&lt;p&gt;  SVM 的目标是找出能够最大化训练集数据间隔（margin）的最优分类超平面。SVM 是一种监督学习==分类==算法，可以用于预测某个数据所述类别。SVM 也被称为最大间隔分类。如下图左侧为线性分类器，右图为 SVM， 可以看出添加更多的样本点在”街道”外不会影响到判定边界，只有位于”街道”边缘的样本点对判定其到作用个， 这样的样本点称为”支持向量”,支持向量包含着重构分割超平面所需要的全部信息.
  ==SVM对特征缩放比较敏感== 见下图，特征缩放后的判定边界要更优,原因是在计算距离时，不同量纲的特征放到一起来计算可能使结果产生较大偏差， 比如两个向量分别是身高（单位米）、体重（单位公斤）由于二者的量纲不一致导致计算出来的向量就可能有较大偏差。
参考 &lt;a href=&quot;https://my.oschina.net/u/3851199/blog/1944830&quot;&gt;特征缩放对哪些机器学习算法结果有影响&lt;/a&gt;
&lt;img src=&quot;https://segmentfault.com/img/bVbb8Z0?w=568&amp;amp;h=147&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;对于下面的训练数据， 见下图，绘制了男人和女人的身高体重散点图:
&lt;img src=&quot;https://upload-images.jianshu.io/upload_images/30697-9e58434c09dea5c3.png?imageMogr2/auto-orient/&quot; alt=&quot;&quot; /&gt;
需要使用SVM来预测如下问题：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;给定一个人的身高、体重，判断这个人是男人还是女人？
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;2-分类超平面&quot;&gt;2. 分类超平面&lt;/h3&gt;
&lt;p&gt;感知机里面已经了解了什么是分类超平面，SVM也是通过分类超平面来对数据做分类，&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;一维情况下对应一个点
二维情况下对应一条直线
三维情况下对应一个平面
N维对应的就是分类超平面 对于上面的判断性别问题， 可以用如下的直线作为分类超平面： ![](https://upload-images.jianshu.io/upload_images/30697-1eaa7027f35dc6f5.png?imageMogr2/auto-orient/)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;3-最优分类超平面&quot;&gt;3. 最优分类超平面&lt;/h3&gt;
&lt;p&gt;SVM学习的目标是能够找到最好的分类超平面，对于上例的性别预测问题，理想情况下对每一个数据都能够正确区分性别。对于给定的数据集，存在多个分类超平面，如下图：
&lt;img src=&quot;https://upload-images.jianshu.io/upload_images/30697-c1810657e03f2350.png?imageMogr2/auto-orient/&quot; alt=&quot;&quot; /&gt;
上面这些分类超平面（对应的绿色、黑车、红色和灰色的线），对于样本数据都能够正确完成分类工作，但是对于真实数据表现各不相同， 比如绿色的分类超平面对于下面这些数据就不能很好的区分：
&lt;img src=&quot;https://upload-images.jianshu.io/upload_images/30697-e164fa0c28f3188b.png?imageMogr2/auto-orient/&quot; alt=&quot;&quot; /&gt;
分析出现错误的原因， 可以发现如果选择了一个靠近某一类数据的超平面，有可能对于靠近的这类数据的分类效果不够理想，因此，SVM算法尝试选择一个==尽可能远离每一种类别数据点的超平面==， 见下图：
&lt;img src=&quot;https://upload-images.jianshu.io/upload_images/30697-5ff13b39686064f9.png?imageMogr2/auto-orient/&quot; alt=&quot;&quot; /&gt;
从上图可以看出，相比于绿线，黑线的分类效果更准确，因此我们可以得到 SVM 寻找最优分类超平面的目标：
    1. 正确的分类训练数据
    2. 更重要的是正确的预测(分类)未知数据&lt;/p&gt;
&lt;h3 id=&quot;最优超平面的选择margin&quot;&gt;最优超平面的选择–Margin&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://upload-images.jianshu.io/upload_images/30697-61ebfe0f810e4c39.png?imageMogr2/auto-orient/&quot; alt=&quot;&quot; /&gt;
如上图是我们选择的最优超平面，图中的 margin（也叫&lt;strong&gt;间隔&lt;/strong&gt;）是超平面距离最近的一个点的距离乘2. Margin 可以理解为一个”无人区”，在 Margin 内部不存在任何数据点。但是这样的超平面有多个，我们如何找到最优的超平面？
&lt;img src=&quot;https://upload-images.jianshu.io/upload_images/30697-8b8d4b02007b65ad.png?imageMogr2/auto-orient/&quot; alt=&quot;&quot; /&gt;
如上图的Margin B 相比前图的Margin A要小的多，但是直觉来看，Margin A的分类效果更好，从图中观察到：
    1. 如果超平面十分接近某个数据点， 那么他的 Margin 就会很小
    2. 超平面距离数据点越远，Margin 越大
这意味着，最优超平面是拥有最大 Margin 的那个超平面；而 SVM 学习的目标就是==找到最大化训练集数据间隔的最优分类超平面==&lt;/p&gt;
&lt;h3 id=&quot;margin-如何计算&quot;&gt;Margin 如何计算&lt;/h3&gt;
&lt;p&gt;先来看超平面的定义：
&lt;script type=&quot;math/tex&quot;&gt;w^Tx = 0&lt;/script&gt;
其中$w^T$是向量，比如二维向量的情况 
&lt;script type=&quot;math/tex&quot;&gt;\begin{aligned} y=ax+b \Rightarrow y-ax-b=0 \Rightarrow w=(-b, -a, 1), x=(1,x,y)\end{aligned}&lt;/script&gt; 
Margin 可以理解为超平面外任意一个点到超平面的投影点的距离，可以先考虑二维的情况（再扩展到N维向量），假设下图的$y$为分类超平面，$x$ 超平面外的数据点，则 Margin 为$x$在分类超平面上的投影点的距离$x$的距离
已知向量$x=(x_1,x_2),y=(y_1,y_2)$，夹角为$\theta$, 下面证明公式(1)成立 : &lt;script type=&quot;math/tex&quot;&gt;\begin{aligned}x\cdot y= \left\|x\right\|\left\|y\right\|\cos(\theta)\end{aligned}&lt;/script&gt;如下图：
&lt;img src=&quot;https://i1.wp.com/www.svm-tutorial.com/wp-content/uploads/2014/11/11-dot-product-angles.png?zoom=2&amp;amp;resize=350%2C273&amp;amp;ssl=1&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;
$\theta=\beta-\alpha$ 因此计算$\cos(\theta)=\cos(\beta-\alpha)=\cos(\beta)\cos(\alpha)+\sin(\beta)\sin(\alpha)$
$\begin{aligned} \begin{split} \cos(\beta)=\frac{x_1}{\left|x\right|} \ \sin(\beta)=\frac{x_2}{\left|x\right|} \\cos(\alpha)=\frac{y_1}{\left|y\right|} \ \sin(\alpha)=\frac{y_2}{\left|y\right|}\end{split} \end{aligned}$
代数公式(2)：
&lt;script type=&quot;math/tex&quot;&gt;\begin{aligned} \begin{split} \cos(\theta)=\cos(\beta-\alpha)=\cos(\beta)\cos(\alpha)+\sin(\beta)\sin(\alpha) \\ = \frac{x_1}{\left\|x\right\|}\frac{y_1}{\left|y\right\|} + \frac{x_2}{\left\|x\right\|}\frac{y_2}{\left|y\right\|}=\frac{x_1y_1+x_2y_2}{\left\|x\right\|\left|y\right\|}=\frac{x \cdot y}{\left\|x\right\|\left|y\right\|}  \end{split} \end{aligned}&lt;/script&gt;
我们要计算的 Margin 就是下图的$x-z$, 
&lt;img src=&quot;https://i0.wp.com/www.svm-tutorial.com/wp-content/uploads/2014/11/14-projection-3-e1415553165199.png?zoom=2&amp;amp;resize=350%2C244&amp;amp;ssl=1&quot; alt=&quot;&quot; /&gt;
由于：
&lt;script type=&quot;math/tex&quot;&gt;\begin{aligned} \begin{split} \cos(\theta)=\frac{\left\|z\right\|}{\left\|x\right\|} \end{split} \end{aligned}&lt;/script&gt;&lt;script type=&quot;math/tex&quot;&gt;\begin{aligned} \begin{split} \cos(\theta)=\frac{x \cdot y}{\left\|x\right\|\left|y\right\|} \end{split} \end{aligned}&lt;/script&gt;&lt;script type=&quot;math/tex&quot;&gt;\begin{aligned} \begin{split} \left\|z\right\|=\frac{x \cdot y}{\left\|y\right\|} \end{split} \end{aligned}&lt;/script&gt;
定义y的方向向量 $u=\frac{y}{\left|y\right|}$，可以得到 &lt;script type=&quot;math/tex&quot;&gt;\begin{aligned}  \left\|z\right\|=u\cdot x\\ u=\frac{z}{\left\|z\right\|}\\z=\left\|z\right\|u \end{aligned}&lt;/script&gt;
从前面的推导可以得出向量 x 在 y上的投影的的计算公式为：
&lt;script type=&quot;math/tex&quot;&gt;\begin{aligned} z=(u \cdot x)u\end{aligned}&lt;/script&gt;从而可以通过计算 $\left|x-z\right|$ 得到Margin&lt;/p&gt;

&lt;p&gt;换一个角度， 假设$x_p$为任意样本点$x$在超平面$wx+b=0$上投影的点，r是x到超平面的集合距离(几何间隔), 可以表示为 $x=x_p+r\frac{w}{\left|w\right|}$, 设$g(x)=w^Tx+b$，由定义可知 &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{aligned} &amp; g(x_p)=0  \\ &amp; g(x)=w^Tx+b=r\left\|w\right\| \end{aligned} %]]&gt;&lt;/script&gt; , $g(x)$实际上度量了样本点 x 到超平面的距离， 在$\left|w\right|$恒定情况下，$g(x)$绝对值的大小反应了几何间隔 $r$的大小，$g(x)$叫做函数间隔。&lt;/p&gt;

&lt;h3 id=&quot;寻找最优分类超平面&quot;&gt;寻找最优分类超平面&lt;/h3&gt;
&lt;p&gt;SVM 训练的目标是==最大化训练集数据间隔==, 因此寻找最优分类超平面等价于找到==最大 Margin(间隔)==， SVM 学习的问题可以理解为线性分类问题基础上，添加如下约束条件：
&lt;script type=&quot;math/tex&quot;&gt;\begin{aligned}  w\cdot x_i + b \geq 1, for\  x_i\  having\ the \ class\ 1 \\ y_i(w\cdot x_i + b) \geq y_i \cdot 1 == 1\end{aligned}&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;\begin{aligned} w\cdot x_i + b \leq -1, for\  x_i\  having\ the \  class\ -1  \\  y_i(w \cdot x_i + b) \geq y_i \cdot -1 == 1 \end{aligned}&lt;/script&gt;
==计算两个超平面距离==
令$H_0: w\cdot x + b = -1$, $H_1: w\cdot x + b =  1$, m为Margin(间距), $u=\frac{w}{\left|w\right|}$, 向量k为垂直于$H_1,H_0$的向量，$k=mu$，见图：
&lt;img src=&quot;https://i2.wp.com/www.svm-tutorial.com/wp-content/uploads/2015/06/svm_margin_demonstration_7.png?w=720&amp;amp;ssl=1&quot; alt=&quot;&quot; /&gt;
&lt;script type=&quot;math/tex&quot;&gt;w\cdot z_0 + b = 1 \Rightarrow w\cdot (x_0+k) + b =1 \\\Rightarrow w \cdot(x_0+m\frac{w}{\left\|w\right\|}) + b = 1 \\\Rightarrow w\cdot x_0 + b = 1 - m\left\|w\right\| = -1 \\\Rightarrow m = \frac{2}{\left\|w\right\|}&lt;/script&gt;
从上面推导，可以看到Margin(间隔)等于 法向量的L2范数的倒数，因此可以得到优化目标为最大化 $m=\frac{2}{\left|w\right|}$，也就是最小化$\left|w\right|$
等价于：
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{aligned} \min &amp;\quad \frac{1}{2}{\left\|w\right\|}^2 \\ s.t. &amp;\quad y_i(w^Tx_i + b) \geq 1\ (i=1,2,...m) \end{aligned} %]]&gt;&lt;/script&gt;
为啥是优化 $\min \frac{1}{2}{\left|w\right|}^2$ 而不是优化 $\min {\left|w\right|}$, 因为后者不好求解， 而 ${\left|w\right|}^2$ 是凸优化问题，容易求解，前面的系数 $\frac{1}{2}$ 是为了求解方便添加, 上面的问题可以转换为拉格朗日对偶问题，将约束条件和原始问题放入同一个等式求解。&lt;/p&gt;
&lt;h3 id=&quot;软间隔&quot;&gt;软间隔&lt;/h3&gt;
&lt;p&gt;当样本线性不可分时，引入松弛向量使得函数间隔加上松弛向量大于等于1. 可以将学习问题构造为规划问题（也称线性支持向量机). 其中C&amp;gt;0为惩罚系数，用来调节对误分类的惩罚大小，最小化目标函数需要使得两项都尽量小， 而 C 用来调节两项关系的
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{aligned}\min\limits_{w,b,\xi} &amp; \quad \frac{1}{2}{\left\|w\right\|}^2+C\sum\limits_{i=1}^N\xi _i \\ s.t. &amp; \quad y_i(w\cdot x_i+b) \geq 1 - \xi,&amp; \quad i=1,2,...N\\ &amp;\quad \xi \geq 0, &amp;\quad i=1,2,...N\end{aligned} %]]&gt;&lt;/script&gt;&lt;/p&gt;
&lt;h3 id=&quot;核技巧&quot;&gt;核技巧&lt;/h3&gt;
&lt;p&gt;核技巧基本思想是通过一个非线性变换将&lt;strong&gt;输入空间&lt;/strong&gt;映射到一个&lt;strong&gt;特征空间&lt;/strong&gt;， 使得输入空间的超曲面模型对应特征空间的一个超平面模型，然后在特征空间中利用线性支持向量机进行求解。在线下支持向量机的对偶问题中，利用核函数$K(x,z)$ 替代内积，求解得到的就是非线性支持向量机
&lt;script type=&quot;math/tex&quot;&gt;f(x)=sign(\sum_{i=1}^{N}{\alpha _i}^*y_iK(x,x_i)+b^*)&lt;/script&gt;&lt;/p&gt;
&lt;h3 id=&quot;损失函数优化&quot;&gt;损失函数优化&lt;/h3&gt;
&lt;p&gt;参考公式(18)、（19） 通过拉格朗日函数将优化目标转化为无约束优化函数：
&lt;script type=&quot;math/tex&quot;&gt;\begin{aligned} L(w,b,a)=\frac{1}{2}{\left\|w\right\|}^2 - \sum\limits_{i=1}^{m}\alpha _i[ y_i(w^Tx_i+b)-1], \alpha _i\geq 0 \end{aligned}&lt;/script&gt; 由于引入了拉格朗日算子，优化目标变为:
&lt;script type=&quot;math/tex&quot;&gt;\underbrace\min_{w,b} \underbrace\max_{a_i\geq0} L(w,b,a)&lt;/script&gt;
拉格朗日对偶参考:&lt;a href=&quot;https://zouwan.github.io/2018/11/%E5%AF%B9%E5%81%B6durality/&quot;&gt;对偶durality&lt;/a&gt;
该优化函数满足 KKT 条件， 可以通过拉格朗日对偶将优化问题转化为等价的对偶问题来求解：&lt;script type=&quot;math/tex&quot;&gt;\underbrace\max_{a_i\geq0} \underbrace\min_{w,b} L(w,b,a)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;$\hat\gamma \overline\gamma \widetilde\gamma \dot\gamma$&lt;/p&gt;

&lt;p&gt;参考:
&lt;a href=&quot;https://www.svm-tutorial.com/2017/02/svms-overview-support-vector-machines/&quot;&gt;svm tutorial&lt;/a&gt;
&lt;a href=&quot;http://blog.pluskid.org/?p=632&quot;&gt;支持向量机: Maximum Margin Classifier&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 28 Nov 2018 00:00:00 +0800</pubDate>
        <link>/2018/11/SVM/</link>
        <guid isPermaLink="true">/2018/11/SVM/</guid>
        
        
      </item>
    
      <item>
        <title>对偶durality</title>
        <description>&lt;h2 id=&quot;什么是对偶&quot;&gt;什么是对偶&lt;/h2&gt;
&lt;p&gt;  两个黄鹂鸣翠柳，一行白鹭上青天， 这两句诗就是对偶：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;两 &amp;lt;-&amp;gt; 一
个 &amp;lt;-&amp;gt; 行
黄鹂 &amp;lt;-&amp;gt; 白鹭
鸣 &amp;lt;-&amp;gt;上
翠柳 &amp;lt;-&amp;gt; 青天
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;  从上面的对偶诗句可以看出来， 对偶问jj题就好比一个线性空间V每一个向量, 在另外一个线性空间$V^&lt;em&gt;$都存在一个对应的映射向量。$V^{&lt;strong&gt;}$ 则是 $V^{*}$的对偶， V 和$V^{&lt;/strong&gt;}$是标准同构的; V 和$V^&lt;/em&gt;$是同构（不标准同构）。
在优化理论中， 目标函数$f(x)$有多种形式，如果目标函数和约束条件都是变量 $x$的线性函数，称为&lt;strong&gt;线性规划&lt;/strong&gt;; 如果目标函数是二次函数，约束条件是线性函数，该优化问题是二次规划； 如果目标函数和约束条件都是非线性函数，该优化为非线性规划；每个规划问题都有一个对应的对偶问题，对偶问题有良好的性质：
    1. 对偶问题的对偶是原问题
    2. 无论原始问题是否凸，对偶问题都是凸优化问题
    3. 对偶问题可以给出原始问题的一个下界
    4. 当满足一定条件是，原始问题与对偶问题的解释完全等价
下面是一个原始问题非凸，对偶问题凸的例子：
    &lt;script type=&quot;math/tex&quot;&gt;\min\limits_x(x^4-50x^2+100x)\\s.t x \geq 4.5&lt;/script&gt;
    &lt;img src=&quot;http://images2015.cnblogs.com/blog/743682/201608/743682-20160801143803840-1888076185.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;为什么要研究对偶问题&quot;&gt;为什么要研究对偶问题&lt;/h3&gt;
&lt;p&gt;  对偶问题优化问题中非常重要的方法，类似文学中的对偶，也是一种配对方式，将数学结构 A 转换为另一种对等的数学结构 B。在优化问题中，可以将非凸优化问题转化为凸优化问题进行求解。&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;判断原问题是否有最优解， 若对偶问题有最优解，则原问题有最优解。&lt;/li&gt;
  &lt;li&gt;对偶问题往往比原问题更简单，更容易判断是否有最优解，更容易求解
    &lt;ol&gt;
      &lt;li&gt;比如 Svm 对偶问题
        &lt;ol&gt;
          &lt;li&gt;求解原问题 $y=wx+b$, 求解算法复杂度与样本维度（等于权值 w 的维度）有关；&lt;/li&gt;
          &lt;li&gt;对偶问题 $L(a)$， 求解算法复杂度与样本数量（等于拉格朗日算子a的数量）有关；当特征维度很高时， 求解原问题复杂度会非常大， 而对偶问题就相对容易了。&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;对于感知机
        &lt;ol&gt;
          &lt;li&gt;原问题在每一轮迭代至少都要判断某个输入实例是否误判点，即对于 $x_i, y_i$，是否有$y_i(wx_i+b) \leq 0$. 运算量集中在计算输入实例$x_i$和权值向量$w$的内积，时间复杂度为$\Theta(N)$, 计算复杂度跟特征维度相关，当特征维度很高时，运算极慢&lt;/li&gt;
          &lt;li&gt;对偶形式输入实例$(x_i,y_i)$是否误判的条件变换为$y_i(\sum_{j=1}{N}a_jy_jx_jx_i + b) \leq 0$ 输入实例以内积形式出现，可以预先计算两两之间的内积（即 Gram 矩阵），这样每次做误判检测时直接从 Gram 矩阵查表得到内积$x_jx_i$, 所以这个误判检测的时间复杂度是$\Theta(N)$， 其中 N 为训练样本数量， n 为特征维度&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;考虑下面的线性规划问题（LP），如何求得函数的下界：
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{alignat}{2} \min_{x,y} &amp; \quad x+3y \\ s.t &amp; \quad x+y &amp; \geq &amp; 2 \\ &amp; \quad x,y &amp; \geq &amp; 0\end{alignat} %]]&gt;&lt;/script&gt;
考虑图解法： 分别绘制 $x+3y=0$（蓝线), $x+y=2$(黑线);通过移动蓝线位置，是的蓝线右上方的所有点满足点x&amp;gt;0, y&amp;gt;0的约束。蓝线移动到红线位置得到最优解2，黄线位置 $x+3y=0.6$ 则不满足约束, （x,y）=(0,0.2)不满足 $x+y \geq 2$
&lt;img src=&quot;/assets/images//15432973484100.jpg&quot; alt=&quot;&quot; /&gt;
  通过作图，可以获得上述LP 问题的最小值2. 换一个角度，如果我们利用以下变换获得目标函数的最小值:
    &lt;script type=&quot;math/tex&quot;&gt;\begin{alignat}{2} x+y \geq  2 \\ + \quad 2y \geq 0 \\ = \quad x+3y \geq 2 \end{alignat}&lt;/script&gt;
从变化后的目标函数得出， x+3y 的最小值为2。 对上述LP问题泛化， 可以得到：
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{alignat}{2} \min_{x,y} &amp; \quad px+qy \\ s.t. &amp; \quad x+y \geq 2 \\  &amp; \quad  x,y \geq 0 \end{alignat} %]]&gt;&lt;/script&gt;
类似于上面的变换方法， 可以获得:
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{alignat}{2} &amp; \quad a(x+y) \geq 2a \\ &amp; + \quad bx \geq 0 \\ &amp; + \quad cy \geq 0 \\ &amp; = \quad (a+b)x+(a+c)y \geq 2a \end{alignat} %]]&gt;&lt;/script&gt;
可以指定 $a+b=p$, $a+c=q$, 则上述问题最优解为$2a$, 这里$2a$为$px+qy$的下界，即 $px+qy \geq 2a$ 永远成立。为了下界最大化，找到满足约束条件 $a+b=p$ 和 $a+c=q$的最大值，才能求得$px+qy$的最小值，综上，上述 LP 问题转换为下面形式的最大值：
    &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{alignat}{2}  \max_{a,b,c} &amp; \quad 2a \\ s.t &amp; \quad a+b=p \\ &amp; \quad a+c=q \\ &amp; \quad a,b,c \geq 0 \end{alignat} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;此时， 上述变换为原问题(Primal)的对偶(dual)问题&lt;/p&gt;

&lt;h3 id=&quot;线性规划的对偶问题&quot;&gt;线性规划的对偶问题&lt;/h3&gt;
&lt;p&gt;对于线性规划问题：
    &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{alignat}{2} \min_{x \in \mathbb{R}^n} &amp; \quad c^Tx \\ s.t. &amp; \quad Ax=b \\ &amp; \quad Gx \leq h \end{alignat} %]]&gt;&lt;/script&gt;
转换为：
    &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{alignat}{2} &amp; \quad -u^TAx = -b^Tu \\ + &amp; \quad -v^TGx \geq -h^Tv \\ = &amp; \quad (-A^Tu-G^Tv)^Tx \geq -b^Tu -h^Tv \end{alignat} %]]&gt;&lt;/script&gt;
令c=$-A^Tu-G^Tv$, 得到对偶问题，可以获得 LP 问题解的下界是$-b^Tu -h^Tv $ ：
    &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{alignat}{2} \max_{u \in \mathbb{R}^m, v \in \mathbb{R}^r} &amp; \quad -b^Tu-h^Tv \\ s.t. &amp; \quad -A^Tu-G^Tv = c \\ &amp; \quad v \geq 0\end{alignat} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;h3 id=&quot;拉格朗日函数&quot;&gt;拉格朗日函数&lt;/h3&gt;
&lt;p&gt;根据 LP 对偶问题的构建方法， 可以令
&lt;script type=&quot;math/tex&quot;&gt;\begin{alignat}{2} L(x,u,v)=c^Tx + u^T(Ax-b) + v^T(Gx-h), v \geq 0 \end{alignat}&lt;/script&gt; 由于$Ax-b=0$且$v^T(Gx-h&amp;lt;=0)$, 因此 $L(x,u,v) \leq c^Tx$
假设集合 C 是原问题的解集， $f^\star$为最优解，则对于任意$u,v \geq 0$, 可以得到
&lt;script type=&quot;math/tex&quot;&gt;\begin{alignat}{2} \begin{split} f^\star=\min\limits_{x \in C}C^Tx \geq \min\limits_{x \in C}L(x,u,v) \geq \min\limits_{x}L(x,u,v) \\ \geq \min\limits_x(c + u^TA+v^TG)^Tx - u^Tb-v^Th := g(u,v)  \end{split} \end{alignat}&lt;/script&gt;
其中 $\min\limits_xL(x,u,v)$是关于 x 的线性函数，最小值是$-\infty$, 所以只有当 x 的系数$(c + u^TA+v^TG)=0$,才能保证求得原问题的最小值。此时&lt;script type=&quot;math/tex&quot;&gt;\begin{alignat}{2} g(u,v)=\min\limits_xL(x,u,v)=-b^Tu-h^Tv \end{alignat}&lt;/script&gt; 综上所述， 可以获得$g(u,v)$的一般形式：
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{equation} g(u,v)=\begin{cases} -b^Tu-h^Tv &amp; \quad if\ c=-u^TA-v^TG \\ -\infty &amp; \quad otherwize \end{cases}\end{equation} %]]&gt;&lt;/script&gt;
最大化函数$g(u,v)$,可以获得原函数的紧致下界（tightest bound), 与LP 对偶问题的定义一致。拉格朗日变换可以适用于任意优化问题上（甚至非凸优化问题）。可以根据有约束的优化问题获得函数$L(x,u,v)$,然后求解$\min\limits_xL(x,u,v)$得到函数$g(u,v)$,即为原问题的对偶问题。
对于一般性有约束优化问题，数学描述如下：
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{alignat}{3}\min\limits_x &amp; \quad f(x) \\ s.t. &amp; \quad h_i(x) \leq 0, i=1,...,m \\ &amp; \quad l_i(x)=0,j=1,...,r\end{alignat} %]]&gt;&lt;/script&gt;
无论$f(x)$是否为凸函数，都可以定义&lt;b&gt;拉格朗日函数(Lagrangian)为：&lt;/b&gt;
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{alignat}{2} L(x,u,v)=\begin{cases} f(x)+\sum\limits_{i=1}^mu_ih_i(x)+\sum\limits_{j=1}{r}v_jl_j(x),&amp;\quad u \geq 0 \\ -\infty, &amp; \quad for \quad u &lt; 0 \end{cases} \end{alignat} %]]&gt;&lt;/script&gt;
假设集合 C 为原问题的解集合，$f^\star$为最优解，对于任意$x$, 最小化函数$L(x,u,v)$可以获得函数下界：&lt;script type=&quot;math/tex&quot;&gt;f^\star=\min\limits_{x\in C}c^Tx \geq \min\limits_{x \in C}L(x,u,v) \geq \min\limits_{x}L(x,u,v):=g(u,v)&lt;/script&gt;
函数$g(u,v)$为==拉格朗日对偶函数(Lagrange dual function)==,可行解$u,v\ u \geq 0$给定了$f^\star$的下界,最大化$g(u,v)$，可获得$f^\star$的逼近解。&lt;/p&gt;
&lt;h3 id=&quot;对偶函数实例二次规划&quot;&gt;对偶函数实例：二次规划&lt;/h3&gt;
&lt;p&gt;用拉格朗日对偶函数解决二次函数优化问题， 定义二次规划
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{alignat}{2} \begin{split} \min\limits_x &amp; \quad \frac{1}{2}x^TQx + c^Tx \quad Q \succ 0 \\ s.t &amp; \quad Ax=b \\ &amp; \quad x \geq 0 \end{split} \end{alignat} %]]&gt;&lt;/script&gt; 对应的拉格朗日函数为&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{alignat}{2} \begin{split} L(x,u,v) &amp; \quad =\frac{1}{2}x^TQx + c^Tx -u^Tx+v^T(Ax-b) \\ &amp; \quad =\frac{1}{2}x^TQx + (c+v^TA-u)^Tx - v^Tb \end{split} \end{alignat} %]]&gt;&lt;/script&gt; 
让 L 对 x 求偏导并等于0, ： &lt;script type=&quot;math/tex&quot;&gt;\lambda=c+v^TA-u \\\nabla_xL(x,u,v)=x^TQ+\lambda^T=0\\ x^T=-\lambda^TQ^{-1}, x=-(Q^{-1})^T\lambda&lt;/script&gt;
得到, L的最小值:
&lt;script type=&quot;math/tex&quot;&gt;\begin{alignat}{0}  g(u,v)=\min_{x \in \mathbb{R}}L(x,u,v)=\\\frac{1}{2} (-\lambda^T Q^{-1})Q(- (Q^{-1})^T\lambda) + \lambda^T (-(Q^{-1})^T\lambda) - v^Tb =\\-\frac{1}{2}\lambda^T(Q^{-1})^T\lambda - v^Tb = -\frac{1}{2}\lambda(Q^{-1})\lambda^T - b^Tv , \quad v\geq 0 \end{alignat}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;参考：
&lt;a href=&quot;http://www.hanlongfei.com/convex/2015/11/05/duality/&quot;&gt;凸优化-对偶问题&lt;/a&gt;
&lt;a href=&quot;https://www.cnblogs.com/ooon/p/5723725.html&quot;&gt;拉格朗日对偶&lt;/a&gt;
&lt;a href=&quot;http://www.hanlongfei.com/convex/2015/11/08/kkt/&quot;&gt;凸优化-KKT条件&lt;/a&gt;
&lt;a href=&quot;https://www.cnblogs.com/dreamvibe/p/4349886.html&quot;&gt;写在SVM之前——凸优化与对偶问题&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 27 Nov 2018 00:00:00 +0800</pubDate>
        <link>/2018/11/%E5%AF%B9%E5%81%B6durality/</link>
        <guid isPermaLink="true">/2018/11/%E5%AF%B9%E5%81%B6durality/</guid>
        
        
      </item>
    
      <item>
        <title>梯度下降法</title>
        <description>&lt;h2 id=&quot;梯度下降法&quot;&gt;梯度下降法&lt;/h2&gt;
</description>
        <pubDate>Mon, 26 Nov 2018 00:00:00 +0800</pubDate>
        <link>/2018/11/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/</link>
        <guid isPermaLink="true">/2018/11/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/</guid>
        
        
      </item>
    
      <item>
        <title>Perception</title>
        <description>&lt;h3 id=&quot;1-什么是感知机perception&quot;&gt;1. 什么是感知机(Perception)？&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;b&gt;感知机&lt;/b&gt;(perceptron)是二类分类的线性分类模型,其输入为实例的特征向量,输出为实例的类别,取+1和-1二值.
    &lt;ul&gt;
      &lt;li&gt;感知机对应于输入空间中将实例划分为正负两类的分离超平面，属于 &lt;b&gt;判别模型&lt;/b&gt;。&lt;/li&gt;
      &lt;li&gt;感知机学习旨在求出将训练数据进行线性划分的分离超平面，为此导入了基于误分类的损失函数，利用梯度下降法对损失函数进行极小化，求得感知机模型。&lt;/li&gt;
      &lt;li&gt;感知机学习算法具有简单而易于实现的优点，分为原始形式和对偶形式。&lt;/li&gt;
      &lt;li&gt;感知机是神经网络与支持向量机的基础。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;2-感知机模型原理&quot;&gt;2. 感知机模型原理&lt;/h3&gt;
&lt;p&gt;感知机满足输入空间到输出空间的如下函数称为感知机：
    &lt;script type=&quot;math/tex&quot;&gt;f(x)=sign(wx+b)&lt;/script&gt; 其中 w 称为权值(weight，b 称为偏置(bias), sign为激活函数
    &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
sign(x)=\begin{cases} +1,x&gt;0 \\ -1,x&lt;0 \\ \end{cases} %]]&gt;&lt;/script&gt;
   sign(x) 将大于0的分为1， 小于0的分为-1.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;感知机相比逻辑回归主要区别是 &lt;b&gt;激活函数不同&lt;/b&gt;
    &lt;ol&gt;
      &lt;li&gt;感知机激活函数是 sign(x),又称单位阶跃函数&lt;/li&gt;
      &lt;li&gt;逻辑回归（logistic regression)激活函数是 sigmoid&lt;/li&gt;
      &lt;li&gt;sigmoid一般取阈值0.5， 大于0.5的分为1， 小于0.1的分为0&lt;/li&gt;
      &lt;li&gt;逻辑回归也被看做是一种概率估计， 详见逻辑回归(todo)&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;工作原理
    &lt;ol&gt;
      &lt;li&gt;给每一个属性一个权重，对属性和权重乘积求和得到的结果和一个阈值进行比较，可以输出一个二分类结果。比如判定是否能够给张三放贷。&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;感知机的几何解释:
    &lt;ol&gt;
      &lt;li&gt;感知机可以用线性方程表示；
 &lt;script type=&quot;math/tex&quot;&gt;w \cdot x+b=0&lt;/script&gt;&lt;/li&gt;
      &lt;li&gt;几何意义如下图:
 对应于特征空间Rn中的一个超平面S，其中w是超平面的法向量，b是截距。这个超平面将特征空间划分为两个部分，位于两部分的点分别被分为正、负两类。因此，超平面S称为分离超平面（separating hyperplanes）
 &lt;img src=&quot;https://pic2.zhimg.com/80/v2-7bd3d267a3d50a511b4b51aace348301_hd.png&quot; alt=&quot;image&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;啥？超平面 （黑脸问号)
        &lt;ol&gt;
          &lt;li&gt;超平面在二维空间里就是直线，方程是a&lt;em&gt;x+b&lt;/em&gt;y+c=0&lt;/li&gt;
          &lt;li&gt;超平面在三维空间里就是平面，方程是a&lt;em&gt;x+b&lt;/em&gt;y+c*z+d=0&lt;/li&gt;
          &lt;li&gt;在n维空间里推广就是就是a&lt;em&gt;x+b&lt;/em&gt;y+c*z+……..+k=0
(a,b,c…)对应向量w, 是由平面确定的数，
   （x,y,z..)是平面上任一点的坐标，对应方程的向量x&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;假定 $wx+b=0$是一个超平面，令$g(x)=wx+b$,即超平面上的所有点都满足$g(x)=0$. 对于超平面的一侧点满足 $g(x)&amp;gt;0$, 另一侧满足 $g(x)&amp;lt;0$.&lt;/li&gt;
      &lt;li&gt;对于不在超平面上的点x到超平面的距离是 $ r=\frac {g(x)}{\left|w \right|}, w为L2范数$
 证明：见下图，其中 O 为原点， $X_p$为超平面上的点， X 是超平面外的点， w 为超平面法向量
&lt;img src=&quot;https://img-blog.csdn.net/20171015173343056?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSmFzdGVyX3dpc2RvbQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center&quot; alt=&quot;image&quot; /&gt;&lt;/li&gt;
    &lt;/ol&gt;

    &lt;p&gt;(1) $X=X_p+r * \frac{w}{ \left|w\right|}$&lt;/p&gt;
    &lt;blockquote&gt;
      &lt;p&gt;向量基本运算法则：$ OX = OX_p + X_pX$; w为法向量，所以 $\frac{w}{\left|w\right|}$ 是垂直于超平面的单位向量&lt;/p&gt;
    &lt;/blockquote&gt;

    &lt;p&gt;(2) $g(x) = w^T(x_p+r\frac{w}{\left|w\right|})+b = (w^T * x_p + b) + r * \frac{w^T * w}{\left|w\right|} = r * \left| w \right|$&lt;/p&gt;
    &lt;blockquote&gt;
      &lt;p&gt;等式（1）带入等式 $g(x)=wx+b$, 由于$X_p$在超平面， 所以 $g(X_p) = w^T * x_p + b = 0$&lt;/p&gt;
    &lt;/blockquote&gt;

    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; 结论一得证
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;3-感知机学习目标&quot;&gt;3. 感知机学习目标&lt;/h3&gt;
&lt;p&gt;  学习参数w与b，确定了w与b，图上的直线（高维空间下为超平面）也就确定了; 感知机的学习目标是找到损失函数,转化为最优化问题&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;思路一：误分类点数目
    &lt;ol&gt;
      &lt;li&gt;无法函数化表示&lt;/li&gt;
      &lt;li&gt;w, b 不是连续可导&lt;/li&gt;
      &lt;li&gt;由于上述1，2两点，思路一不可行，考虑思路二&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;思路二：误分类点距离超平面距离，总距离越小越好
 &lt;script type=&quot;math/tex&quot;&gt;\frac{1}{\| w\|} | w \cdot x_0 + b |&lt;/script&gt;
    &lt;ol&gt;
      &lt;li&gt;当数据点被误分类(+1吧被误判为-1), 则 $(w \cdot x_0 + b) &amp;lt; 0 $, 此时预测值 $y_i$=+1, 满足 &lt;script type=&quot;math/tex&quot;&gt;-y_i(w \cdot x_0 + b) &gt; 0&lt;/script&gt;&lt;/li&gt;
      &lt;li&gt;当数据点被误分类(-1吧被误判为+1), 则 $(w \cdot x_0 + b) &amp;gt; 0 $, 此时预测值 $y_i$=-1, 满足 &lt;script type=&quot;math/tex&quot;&gt;y_i(w \cdot x_0 + b) &gt; 0&lt;/script&gt;&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;损失函数确定
    &lt;ol&gt;
      &lt;li&gt;
        &lt;p&gt;上式不考虑$|w|$, 去掉后面的绝对值，得到损失函数:
 &lt;script type=&quot;math/tex&quot;&gt;J(w,b) = -\sum_{x_i\in M}{y_i(w \cdot x_i + b)}，M为误分类点集&lt;/script&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;感知机不关心实际超平面距离每个点的距离（定量，误分类点的距离），只关心最终分类是否正确（定性，误分类点的个数）&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;损失函数推导
    &lt;ol&gt;
      &lt;li&gt;把正例预测为负例： $w*x_i+b&amp;gt;0时， y_i=-1$&lt;/li&gt;
      &lt;li&gt;把负例预测为正例： $w*x_i+b&amp;lt;0时， y_i=1$&lt;/li&gt;
      &lt;li&gt;可以得到误分类点到超平面总的距离是（注意， 只考虑不在超平面的点）
 &lt;script type=&quot;math/tex&quot;&gt;-\frac{1}{\left\|w\right\|}\sum_{x_i \in M}{y_i*(w*x_i+b)}&lt;/script&gt; 
 其中$\left|w\right|$可以忽略（原因1：$\frac{1}{\left|w\right|}$ 不影响正负的判断，即不影响学习算法的中间过程我们；原因2：感知机训练终止条件是所有样本正确分类, 即不存在误分类点，因此$\frac{1}{\left|w\right|}$ 对最终结果无影响），得到损失函数：
 &lt;script type=&quot;math/tex&quot;&gt;-\sum_{x_i \in M}{y_i*(w*x_i+b)}&lt;/script&gt;&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;更新策略（梯度下降）
    &lt;ol&gt;
      &lt;li&gt;对损失函数计算梯度（梯度下降）
&lt;script type=&quot;math/tex&quot;&gt;\left\{ \begin{array}{lr} \nabla_{w}{L(w,b)} = -\sum\limits_{x \in M}{y_i * x_i} \\ \nabla_{b}{L(w,b)} = -\sum\limits_{x \in M}{y_i} \end{array} \right.&lt;/script&gt;&lt;br /&gt;
使用批梯度（选取所有误分类点取平均对 w, b 进行更新)， 更新公式如下：
&lt;script type=&quot;math/tex&quot;&gt;\left\{ \begin{array}{lr} w \leftarrow w+\eta \frac{1}{N}\sum\limits_{x^{(i)},y^{(i)} \in M}y_i*x_i \\ b \leftarrow b+\eta \sum\limits_{y^{(i)} \in M }y_i  \end{array} \right.&lt;/script&gt;
使用随机梯度（随机选取误分类点$(x_i, y_i)$对 w, b 进行更新)， 更新公式如下：
&lt;script type=&quot;math/tex&quot;&gt;\left\{ \begin{array}{lr} w \leftarrow w+\eta y_i*x_i \\ b \leftarrow b+\eta y_i  \end{array} \right.&lt;/script&gt;&lt;/li&gt;
      &lt;li&gt;如何理解上面更新公式？(from &lt;a href=&quot;https://www.zhihu.com/question/57747902&quot;&gt;知乎&lt;/a&gt;)
        &lt;ol&gt;
          &lt;li&gt;梯度更新公式确实不是推导而是创造出来的，所以只能从概念上去理解&lt;/li&gt;
          &lt;li&gt;设想下有个函数，你的目标是：找到一个参数$\theta$ 使得它的值$Y$最小。但它很复杂，你无法找到这个参数的解析解，所以你希望通过梯度下降法去猜这个参数&lt;/li&gt;
          &lt;li&gt;问题是怎么猜？
 对于多数有连续性的函数来说，显然不可能把每个$\theta$都试一遍。所以只能先随机取一个$\theta$，然后看看怎么调整它最有可能使得$\theta$变小。
 把这个过程重复n遍，自然最后得到的损失函数L的估值会越来越小&lt;/li&gt;
          &lt;li&gt;调整策略（参考文章:&lt;a href=&quot;https://zouwan.github.io/2018/11/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/&quot;&gt;梯度下降法&lt;/a&gt;）
 基于当前我们拥有的那个参数$\theta$，所以有了：&lt;script type=&quot;math/tex&quot;&gt;\theta_{t+1}=\theta_t + \Delta&lt;/script&gt;那现在问题是每次更新的时候这个$\theta$应该取什么值？
 我们知道关于某变量的（偏）导数的概念是指当（仅仅）该变量往 &lt;b&gt;正向&lt;/b&gt; 的变化量趋向于0时的其函数值变化量的极限。若求Y关于$\theta_t$的导数，得到一个值比如+5, 那就说明若现在我们把$\theta_t$往负向移动，损失函数 $J(w,b）$的值会变小，但不一定是正好-5。同理若现在导数是-5，那么把$\theta$往 &lt;b&gt;负向&lt;/b&gt;移动，损失函数$J(w,b)$值会变小。&lt;br /&gt;
 不管导数值$\theta$是正的还是负的(正负即导数的方向), 对于$J(w,b)$来说，$-\theta$的最终方向（即最终的正负号，决定是增(+)还是减(-))一定是能将$J(w,b)$值变小的方向（除非导数为0）。所以有了:
 &lt;script type=&quot;math/tex&quot;&gt;\theta_{t+1}=\theta_t + (-\Delta)&lt;/script&gt;但是说到底，$\Delta$的绝对值只是个关于Y的变化率，本质上和$\theta_t$没关系。所以为了抹去$\Delta$在幅度上对$\theta_t$的影响，需要一个学习率来控制：$\alpha$。所以有了：$\alpha \in \left(0,1\right]$
 &lt;script type=&quot;math/tex&quot;&gt;\theta_{t+1}=\theta_t + (-\alpha \Delta) =\theta_t -\alpha \Delta&lt;/script&gt;就是有多少个参数，就有多少个不同的$\Delta$。
 &lt;br /&gt;现在分析在梯度下降法中最常听到的一句话：“梯度下降法就是朝着梯度的反方向迭代地调整参数直到收敛。” 这里的梯度就是 $\Delta$ ,而梯度的反方向就是 $-\Delta$ 的符号方向—梯度实际上是个向量。所以这个角度来说，即使我们只有一个参数需要调整，也可以认为它是个一维的向量。 
 &amp;lt;br&lt;b&gt;整个过程你可以想象自己站在一个山坡上，准备走到山脚下（最小值的地方），于是很自然地你会考虑朝着哪个方向走，方向由$\Delta$的方向给出，而至于一次走多远，由$|\alpha\Delta|$来控制。这种方式相信你应该能理解其只能找到局部最小值，而不是全局的。&lt;/b&gt;&lt;/li&gt;
          &lt;li&gt;换个角度看，想象损失函数是$y=x^2$的抛物线形式，其梯度为 $y’=2x$
            &lt;ol&gt;
              &lt;li&gt;纵坐标y表示损失函数，横坐标x表示参数 $\theta$&lt;/li&gt;
              &lt;li&gt;$\theta$在最低点左边时，导数小于0，只有$\theta$往右边（最低点方向）走，损失函数才会变小，即$\theta$要往导数的负方向变化
                &lt;ol&gt;
                  &lt;li&gt;当 x=-2; y=-4, 此时往负向损失函数$y$增大，往正向走损失函数$y$减少&lt;/li&gt;
                &lt;/ol&gt;
              &lt;/li&gt;
              &lt;li&gt;$\theta$在最低点右边时，导数大于0，只有$\theta$往左边（最低点方向）走，损失函数才会变小，即$\theta$要往导数的负方向变化
 1.当 x=+2; y=+4, 此时往正向损失函数$y$增大，往负向走损失函数$y$减少          &lt;br /&gt;
 &lt;img src=&quot;/assets/images//15432151219295.jpg&quot; alt=&quot;-w450&quot; /&gt;&lt;/li&gt;
            &lt;/ol&gt;
          &lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;更新方法
 &lt;br /&gt;对于感知机模型 $f(x)=sign(w*x+b)$
        &lt;ol&gt;
          &lt;li&gt;选取初值 $w_0, b_0$&lt;/li&gt;
          &lt;li&gt;在训练集选择数据 $(x_i, y_i)$
            &lt;ol&gt;
              &lt;li&gt;任意抽取数据点，当所有数据点没有误分类时结束算法，否则进入3.&lt;/li&gt;
            &lt;/ol&gt;
          &lt;/li&gt;
          &lt;li&gt;此时有 $y_i(w&lt;em&gt;x_i+b)&amp;lt;0$ ， 更新参数:
 $\left{ \begin{array}{lr} w \leftarrow w+\eta y_i&lt;/em&gt;x_i \ b \leftarrow b+\eta y_i  \end{array} \right. $&lt;/li&gt;
          &lt;li&gt;其中$\eta$ 学习率一般为0-1之间&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;来段代码解析
        &lt;ol&gt;
          &lt;li&gt;例题来自李航《统计学习方法》的感知机部分
 &lt;img src=&quot;https://pic1.zhimg.com/80/v2-0575c3efd5d8e1322aaf1d6ef18808ac_hd.jpg&quot; alt=&quot;&quot; /&gt;&lt;/li&gt;
          &lt;li&gt;例题解析
            &lt;ol&gt;
              &lt;li&gt;构建优化函数： $min_{w,b}J(w,b) = -\sum_{x_i \in M}y_i(w \cdot x + b)$&lt;/li&gt;
              &lt;li&gt;求解w, b, 为了计算方便，设 $\eta=0.1$ （$\eta$为学习率，建议设置 0-1之间）&lt;/li&gt;
            &lt;/ol&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;步骤
     取初值: w=[0 0] b=1
     0:w=[0 0] b=1x=[3 3] 预测正确,跳过
     0:w=[0 0] b=1x=[4 3] 预测正确,跳过
     0:w=[0 0] b=1x=[1 1] 预测错误,更新w,b
     i=0 w=[-1 -1] b=0&lt;/p&gt;

            &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; 1:w=[-1 -1] b=0x=[3 3] 预测错误,更新w,b
 i=1 w=[2 2] b=1
            
 2:w=[2 2] b=1x=[3 3] 预测正确,跳过
 2:w=[2 2] b=1x=[4 3] 预测正确,跳过
 2:w=[2 2] b=1x=[1 1] 预测错误,更新w,b
 i=2 w=[1 1] b=0
            
 3:w=[1 1] b=0x=[3 3] 预测正确,跳过
 3:w=[1 1] b=0x=[4 3] 预测正确,跳过
 3:w=[1 1] b=0x=[1 1] 预测错误,更新w,b
 i=3 w=[0 0] b=-1
            
 4:w=[0 0] b=-1x=[3 3] 预测错误,更新w,b
 i=4 w=[3 3] b=0
            
 5:w=[3 3] b=0x=[3 3] 预测正确,跳过
 5:w=[3 3] b=0x=[4 3] 预测正确,跳过
 5:w=[3 3] b=0x=[1 1] 预测错误,更新w,b
 i=5 w=[2 2] b=-1
            
 6:w=[2 2] b=-1x=[3 3] 预测正确,跳过
 6:w=[2 2] b=-1x=[4 3] 预测正确,跳过
 6:w=[2 2] b=-1x=[1 1] 预测错误,更新w,b
 i=6 w=[1 1] b=-2
            
 7:w=[1 1] b=-2x=[3 3] 预测正确,跳过
 7:w=[1 1] b=-2x=[4 3] 预测正确,跳过
 7:w=[1 1] b=-2x=[1 1] 预测错误,更新w,b
 i=7 w=[0 0] b=-3
            
 8:w=[0 0] b=-3x=[3 3] 预测错误,更新w,b
 i=8 w=[3 3] b=-2
            
 9:w=[3 3] b=-2x=[3 3] 预测正确,跳过
 9:w=[3 3] b=-2x=[4 3] 预测正确,跳过
 9:w=[3 3] b=-2x=[1 1] 预测错误,更新w,b
 i=9 w=[2 2] b=-3
            
 10:w=[2 2] b=-3x=[3 3] 预测正确,跳过
 10:w=[2 2] b=-3x=[4 3] 预测正确,跳过
 10:w=[2 2] b=-3x=[1 1] 预测错误,更新w,b
 i=10 w=[1 1] b=-4
            
 11:w=[1 1] b=-4x=[3 3] 预测正确,跳过
 11:w=[1 1] b=-4x=[4 3] 预测正确,跳过
 11:w=[1 1] b=-4x=[1 1] 预测正确,跳过
 w=[1 1] b= 第11 次更新时得到解
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;            &lt;/div&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;代码&lt;/p&gt;

            &lt;p&gt;```python
     # 李航《统计学方法》p29 例2.1
     # 正例：x1=(3,3), x2=(4,3),负例：x3=(1,1)
     import numpy as np
     import matplotlib.pyplot as plt
     # x取值，样本数据，对应的是一个二维向量
     p_x = np.array([[3, 3], [4, 3], [1, 1]])
     # y取值，样本数据对应的正负标签，-1=负样本， +1=正样本
     y = np.array([1, 1, -1])
     # 输出样本数据，其中正样本为红色，负样本为蓝色
     plt.figure()
     for i in range(len(p_x)):
         if y[i] == 1:
             plt.plot(p_x[i][0], p_x[i][1], ‘ro’)
         else:
             plt.plot(p_x[i][0], p_x[i][1], ‘bo’)
     w = np.array([0, 0])
     b = 1
     delta = 1&lt;/p&gt;

            &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; print(&quot;取初值: w=&quot; + str(w) + &quot; b=&quot; + str(b))
 for i in range(100):
     choice = -1
     for j in range(len(p_x)):
         y_pred = np.sign(np.dot(w, p_x[j]) + b)
         if y[j] != y_pred:
             print(str(i)+&quot;:w=&quot;+str(w)+&quot; b=&quot;+str(b)+&quot;x=&quot;+str(p_x[j])+&quot; 预测错误,更新w,b&quot;)
             choice = j
             break
         else:
             print(str(i)+&quot;:w=&quot;+str(w)+&quot; b=&quot;+str(b)+&quot;x=&quot;+str(p_x[j])+&quot; 预测正确,跳过&quot;)
            
     if choice == -1:
         print(&quot;w=&quot; + str(w) + &quot; b=&quot; + &quot; 第&quot; + str(i) + &quot; 次更新时得到解&quot;)
         break
     w = w + delta * y[choice]*p_x[choice]
     b = b + delta * y[choice]
     print(&quot;i=&quot; + str(i) + &quot; w=&quot; + str(w) +&quot; b=&quot; + str(b) + &quot;\n&quot;)
             
 line_x = [0, 10]
 line_y = [0, 0]
             
 for i in range(len(line_x)):
     line_y[i] = (-w[0] * line_x[i]- b)/w[1]
             
 plt.plot(line_x, line_y)
        
 ```
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;            &lt;/div&gt;
          &lt;/li&gt;
          &lt;li&gt;输出
&lt;img src=&quot;/assets/images//15432216556174.jpg&quot; alt=&quot;&quot; /&gt;&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;对偶形式
感知机算法对偶形式的目的是降低运算量（当特征空间的维度很高时）。 每次感知机梯度下降算法的迭代都是选择一个误分类样本数据更新$w,b$参数。整个迭代过程中，对于从来没有被误分类的样本，其被选择参与$w,b$迭代的次数是0， 假设样本点$(x_{(i)},y_{(i)}) \in M$在迭代更新$w,b$时被使用了$k_i$次，因此在原始感知机算法中，算法最后收敛时，$w,b$为:
 &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{alignat}{2} &amp; w =\eta\sum_{i=1}^{K}k_ix^{(i)}y^{(i)} \\ &amp; b=\eta\sum_{i=1}^{K}k_iy^{(i)} \end{alignat} %]]&gt;&lt;/script&gt;
 将 $w,b$回代原始感知机模型
 &lt;script type=&quot;math/tex&quot;&gt;\begin{alignat}{1} f(x)=sign(wx+b)=sign(\eta\sum_{i=1}^{K}k_ix^{(i)}y^{(i)} \cdot x + \eta\sum_{i=1}^{K}k_iy^{(i)})\end{alignat}&lt;/script&gt;
 此时，学习目标不再是$w,b$,而是$k_i, i=1,2,…,N$, 相应的训练过程为：
    &lt;ol&gt;
      &lt;li&gt;初始时，$\forall n_i=0$&lt;/li&gt;
      &lt;li&gt;在训练集选取数据$(x_i, y_i)$&lt;/li&gt;
      &lt;li&gt;如果$y_i(\sum_{j=1}^{N}n_j\eta y_jx_j \cdot x_i + \sum_{j=1}^{N}n_j\eta y_j) \leq 0 $, 更新 $n_i \leftarrow n_i + 1， i=i+1$&lt;/li&gt;
      &lt;li&gt;转至2. 直至没有误分类数据
可以看到，相比原始形式训练过程区别在于步骤3，样本点的特征向量以内积的形式存在于感知机对偶形式的训练算法中，可以事先计算好所有内积（即 Gram 矩阵），就可以加速训练。&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;
&lt;p&gt;参考：
  1. &lt;a href=&quot;http://www.cnblogs.com/pinard/p/6042320.html&quot;&gt;Pincard-感知机原理小结&lt;/a&gt;&lt;/p&gt;

</description>
        <pubDate>Sun, 25 Nov 2018 00:00:00 +0800</pubDate>
        <link>/2018/11/perception/</link>
        <guid isPermaLink="true">/2018/11/perception/</guid>
        
        
      </item>
    
      <item>
        <title>回归和分类</title>
        <description>&lt;h3 id=&quot;回归&quot;&gt;回归&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;区别
    &lt;ol&gt;
      &lt;li&gt;
        &lt;p&gt;从输出值、目的和评价指标来区分&lt;/p&gt;

        &lt;table&gt;
          &lt;thead&gt;
            &lt;tr&gt;
              &lt;th style=&quot;text-align: center&quot;&gt;区别&lt;/th&gt;
              &lt;th style=&quot;text-align: center&quot;&gt;回归&lt;/th&gt;
              &lt;th style=&quot;text-align: center&quot;&gt;分类&lt;/th&gt;
            &lt;/tr&gt;
          &lt;/thead&gt;
          &lt;tbody&gt;
            &lt;tr&gt;
              &lt;td style=&quot;text-align: center&quot;&gt;输出&lt;/td&gt;
              &lt;td style=&quot;text-align: center&quot;&gt;连续值&lt;/td&gt;
              &lt;td style=&quot;text-align: center&quot;&gt;离散数据&lt;/td&gt;
            &lt;/tr&gt;
            &lt;tr&gt;
              &lt;td style=&quot;text-align: center&quot;&gt;目的&lt;/td&gt;
              &lt;td style=&quot;text-align: center&quot;&gt;寻找最优拟合&lt;/td&gt;
              &lt;td style=&quot;text-align: center&quot;&gt;寻找决策边界&lt;/td&gt;
            &lt;/tr&gt;
            &lt;tr&gt;
              &lt;td style=&quot;text-align: center&quot;&gt;评价方法&lt;/td&gt;
              &lt;td style=&quot;text-align: center&quot;&gt;SSE(sum of square errors)、拟合优度&lt;/td&gt;
              &lt;td style=&quot;text-align: center&quot;&gt;精度、混淆矩阵&lt;/td&gt;
            &lt;/tr&gt;
          &lt;/tbody&gt;
        &lt;/table&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;从应用场景看&lt;/p&gt;
        &lt;ol&gt;
          &lt;li&gt;回归问题的应用场景&lt;br /&gt;
 回归问题通常是用来预测一个值，如预测房价、未来的天气情况等等，例如一个产品的实际价格为500元，通过回归分析预测值为499元，我们认为这是一个比较好的回归分析。一个比较常见的回归算法是线性回归算法（LR）。另外，回归分析用在神经网络上，其最上层是不需要加上softmax函数的，而是直接对前一层累加即可。回归是对真实值的一种逼近预测。&lt;/li&gt;
          &lt;li&gt;分类问题的应用场景
 分类问题是用于将事物打上一个标签，通常结果为离散值。例如判断一幅图片上的动物是一只猫还是一只狗，分类通常是建立在回归之上，分类的最后一层通常要使用softmax函数进行判断其所属类别。分类并没有逼近的概念，最终正确结果只有一个，错误的就是错误的，不会有相近的概念。最常见的分类方法是逻辑回归，或者叫逻辑分类。&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;logistic 回归是分类
    &lt;ol&gt;
      &lt;li&gt;logistic回归只是用到了回归算法，但是其输出的结果是决策边界，是不连续的。&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;逻辑回归5要素
    &lt;ol&gt;
      &lt;li&gt;假设
        &lt;ol&gt;
          &lt;li&gt;数据服从伯努利分布,模型可以描述为
  &lt;script type=&quot;math/tex&quot;&gt;h_\theta(x;\theta) = p&lt;/script&gt;&lt;/li&gt;
          &lt;li&gt;假设正样本的概率是
  &lt;script type=&quot;math/tex&quot;&gt;p = \frac{1}{(1 + e^{-\theta^{T}x})}&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;h_\theta(x;\theta) = \frac{1}{(1 + e^{-\theta^{T}x})}&lt;/script&gt;&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;损失函数
        &lt;ol&gt;
          &lt;li&gt;极大似然函数
 &lt;script type=&quot;math/tex&quot;&gt;\prod_{1}^{m}h_{\theta}(x^i;\theta)^yi * (1 - h_\theta(x^i;\theta))^{1-y^i}&lt;/script&gt;&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;求解方法
        &lt;ol&gt;
          &lt;li&gt;梯度下降求解参数
            &lt;ol&gt;
              &lt;li&gt;批量梯度下降 batch gd&lt;/li&gt;
              &lt;li&gt;随机梯度下降 sgd&lt;/li&gt;
              &lt;li&gt;小批量梯度下降&lt;/li&gt;
            &lt;/ol&gt;
          &lt;/li&gt;
          &lt;li&gt;学习率选择
            &lt;ol&gt;
              &lt;li&gt;先大后小&lt;/li&gt;
              &lt;li&gt;更新频繁的参数
                &lt;ol&gt;
                  &lt;li&gt;选择较小学习率&lt;/li&gt;
                  &lt;li&gt;否则，选择较大学习率&lt;/li&gt;
                &lt;/ol&gt;
              &lt;/li&gt;
            &lt;/ol&gt;
          &lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;目的
        &lt;ol&gt;
          &lt;li&gt;数据二分类&lt;/li&gt;
          &lt;li&gt;提高准确率&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;如何分类
        &lt;ol&gt;
          &lt;li&gt;选定分类阈值&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Fri, 28 Sep 2018 00:00:00 +0800</pubDate>
        <link>/2018/09/%E5%9B%9E%E5%BD%92%E5%92%8C%E5%88%86%E7%B1%BB/</link>
        <guid isPermaLink="true">/2018/09/%E5%9B%9E%E5%BD%92%E5%92%8C%E5%88%86%E7%B1%BB/</guid>
        
        
      </item>
    
      <item>
        <title>基数估计算法</title>
        <description>&lt;p&gt;转自淘宝张洋的基数估计算法概览&lt;/p&gt;

&lt;h3 id=&quot;基数估计算法定义&quot;&gt;基数估计算法定义&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;基数是指一个集合中，不同的数的个数；基数统计是集合不同的数的个数
    &lt;ul&gt;
      &lt;li&gt;比如一个集合｛0, 1, 2, 2, 4, 5}，其基数是5&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;基数估计是估计一个集合中不同数的个数
    &lt;ul&gt;
      &lt;li&gt;用概率算法的思想，来用低空间和时间成本，以一个很低的误差度来估计数据的基数&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;原理&quot;&gt;原理&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;一个简单直观的基数估计方法
  假设你通过如下步骤生成了一个数据集：
    &lt;ol&gt;
      &lt;li&gt;随机生成n个服从均匀分布的数字&lt;/li&gt;
      &lt;li&gt;随便重复其中一些数字，重复的数字和重复次数都不确定&lt;/li&gt;
      &lt;li&gt;打乱这些数字的顺序，得到一个数据集&lt;/li&gt;
    &lt;/ol&gt;

    &lt;p&gt;问题：如何估计这个数据集中有多少不同的数字？&lt;/p&gt;
    &lt;ol&gt;
      &lt;li&gt;找出数据集中最小的数字&lt;/li&gt;
      &lt;li&gt;假如m是数值上限，x是找到的最小的数，则m/x是基数的一个估计&lt;/li&gt;
    &lt;/ol&gt;

    &lt;p&gt;例如，我们扫描一个包含0到1之间数字组成的数据集，其中最小的数是0.01，则一个比较合理的推断是数据集中大约有100个不同的元素，否则我们应该预期能找到一个更小的数。注意这个估计值和重复次数无关：就如最小值重复多少次都不改变最小值的数值&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;改进方案
  上面的简单基数估计存在精度不足的问题&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;优势&quot;&gt;优势&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;基数估计算法使用很少的资源给出数据集基数的一个良好估计&lt;/li&gt;
  &lt;li&gt;这个方法和数据本身的特征无关，而且可以高效的进行分布式并行计算&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;应用场景&quot;&gt;应用场景&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;应用场景
    &lt;ul&gt;
      &lt;li&gt;流量监控（多少不同IP访问过一个服务器）&lt;/li&gt;
      &lt;li&gt;数据库查询优化（例如我们是否需要排序和合并，或者是否需要构建哈希表）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;参考&quot;&gt;参考&lt;/h3&gt;
&lt;p&gt;http://blog.codinglabs.org/articles/cardinality-estimation.html
https://www.jianshu.com/p/a966e7d71666&lt;/p&gt;
</description>
        <pubDate>Thu, 27 Sep 2018 00:00:00 +0800</pubDate>
        <link>/2018/09/%E5%9F%BA%E6%95%B0%E4%BC%B0%E8%AE%A1%E7%AE%97%E6%B3%95/</link>
        <guid isPermaLink="true">/2018/09/%E5%9F%BA%E6%95%B0%E4%BC%B0%E8%AE%A1%E7%AE%97%E6%B3%95/</guid>
        
        
      </item>
    
      <item>
        <title>切单项目小结</title>
        <description>&lt;h3 id=&quot;背景&quot;&gt;背景&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;场站交易线下交易严重。带来了如下问题
    &lt;ol&gt;
      &lt;li&gt;平台收益受损；&lt;/li&gt;
      &lt;li&gt;乘客体验差;&lt;/li&gt;
      &lt;li&gt;容易引发线下超载等安全事故&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;场站切单率.预计至少超过20%
    &lt;ol&gt;
      &lt;li&gt;客服随机电话回访&lt;/li&gt;
      &lt;li&gt;场站订单司机行为分析&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;思路&quot;&gt;思路&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;根据司机历史统计行为进行建模&lt;/li&gt;
  &lt;li&gt;司机在刷订单列表时，根据模型（司机特征、乘客特征、订单特征）预测切单概率&lt;/li&gt;
  &lt;li&gt;对切单倾向严重的匹配做管控
    &lt;ol&gt;
      &lt;li&gt;最严重的直接过滤&lt;/li&gt;
      &lt;li&gt;其他司机按照其平均成交金额做分级管控&lt;/li&gt;
      &lt;li&gt;根据实时供需数据随时调整阈值（doing.&lt;/li&gt;
      &lt;li&gt;从安全角度应该直接清除，从增长角度需要区分场景
        &lt;ol&gt;
          &lt;li&gt;运力不足：适当放松阈值，优先满足乘客乘坐，同时平台有较低概率能够拿到提成&lt;/li&gt;
          &lt;li&gt;运力充足：适当收紧阈值，优先让平台上的优质司机(切单概率低、忠诚度高.接单&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;特征工程&quot;&gt;特征工程&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;数据清洗
    &lt;ol&gt;
      &lt;li&gt;数据质量
        &lt;ol&gt;
          &lt;li&gt;数据完整性–例如人的属性缺少性别、籍贯、年龄等
            &lt;ol&gt;
              &lt;li&gt;数据补全（剔除）&lt;/li&gt;
            &lt;/ol&gt;
          &lt;/li&gt;
          &lt;li&gt;数据唯一性–不同来源数据出现重复情况
            &lt;ol&gt;
              &lt;li&gt;去除重复记录&lt;/li&gt;
            &lt;/ol&gt;
          &lt;/li&gt;
          &lt;li&gt;数据权威性–同一个指标出现多个来源的数据，且数值不一样
            &lt;ol&gt;
              &lt;li&gt;选用最权威渠道数据&lt;/li&gt;
            &lt;/ol&gt;
          &lt;/li&gt;
          &lt;li&gt;数据合法性–获取的数据与尝试不符，如年龄大于200岁
            &lt;ol&gt;
              &lt;li&gt;设定判定规则（或者半人工处理）&lt;/li&gt;
            &lt;/ol&gt;
          &lt;/li&gt;
          &lt;li&gt;数据一致性–不同来源的不同指标，实际内涵是一样的（或统一指标内涵不一致）
            &lt;ol&gt;
              &lt;li&gt;建立数据体系&lt;/li&gt;
            &lt;/ol&gt;
          &lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;数据更适合挖掘
        &lt;ol&gt;
          &lt;li&gt;高纬度
            &lt;ol&gt;
              &lt;li&gt;降维（主成分分析、随机森林）&lt;/li&gt;
            &lt;/ol&gt;
          &lt;/li&gt;
          &lt;li&gt;维度低
            &lt;ol&gt;
              &lt;li&gt;抽象
                &lt;ol&gt;
                  &lt;li&gt;汇总（平均、加总、最大、最小）&lt;/li&gt;
                  &lt;li&gt;离散化、聚类、自定义分组&lt;/li&gt;
                &lt;/ol&gt;
              &lt;/li&gt;
            &lt;/ol&gt;
          &lt;/li&gt;
          &lt;li&gt;无关信息和字段冗余
            &lt;ol&gt;
              &lt;li&gt;剔除&lt;/li&gt;
            &lt;/ol&gt;
          &lt;/li&gt;
          &lt;li&gt;多指标数值、单位不同
            &lt;ol&gt;
              &lt;li&gt;归一化（最小-最大、零-均值、小数定标）&lt;/li&gt;
            &lt;/ol&gt;
          &lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;缺失值处理（https://www.zhihu.com/question/26639110）
    &lt;ol&gt;
      &lt;li&gt;删除。最简单最直接的方法，很多时候也是最有效的方法，这种做法的缺点是可能会导致信息丢失。
        &lt;ol&gt;
          &lt;li&gt;删除有缺失数据的样本&lt;/li&gt;
          &lt;li&gt;删除有过多缺失数据的特征&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;补全。
        &lt;ol&gt;
          &lt;li&gt;用规则或模型将缺失数据补全，这种做法的缺点是可能会引入噪声。平均数、中位数、众数、最大值、最小值、固定值、插值等等&lt;/li&gt;
          &lt;li&gt;建立一个模型来“预测”缺失的数据。（KNN, Matrix completion等方法）
            &lt;ol&gt;
              &lt;li&gt;根本缺陷：如果其他变量与缺失变量无关，则预测结果无意义；反之如果预测结果非常准确，则说明加入这个变量无意义；&lt;/li&gt;
              &lt;li&gt;一般情况下，介于二者之间&lt;/li&gt;
            &lt;/ol&gt;
          &lt;/li&gt;
          &lt;li&gt;引入虚拟变量(dummy variable.来表征是否有缺失，是否有补全。&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;忽略。
        &lt;ol&gt;
          &lt;li&gt;有一些模型，如随机森林，自身能够处理数据缺失的情况，在这种情况下不需要对缺失数据做任何的处理，这种做法的缺点是在模型的选择上有局限。&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;把变量映射到高维空间。
        &lt;ol&gt;
          &lt;li&gt;好处：保留了原始数据全部信息，不用考虑缺失值、不用考虑线性不可分之类问题&lt;/li&gt;
          &lt;li&gt;缺点：计算量太大，而且需要样本量足够大&lt;/li&gt;
          &lt;li&gt;举例：性别有男、女、缺失三种情况，映射为三个变量：是否男、是否女、是否缺失。连续型变量也可以如此处理。&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;缺失值较多
        &lt;ol&gt;
          &lt;li&gt;修改为  “XX字段有值”、”字段为空”&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;特征转化
    &lt;ol&gt;
      &lt;li&gt;&lt;strong&gt;对原始特征转化，把原来的非线性关系转化为线性关系&lt;/strong&gt;
        &lt;ol&gt;
          &lt;li&gt;LinearReg模型是线性模型，在非线性情况下效果不好&lt;/li&gt;
          &lt;li&gt;复杂模型(如Svm)对于高维特征有时间约束&lt;/li&gt;
          &lt;li&gt;CTR 预估目前最常用方法还是 LR&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;方法1:离散化
        &lt;ol&gt;
          &lt;li&gt;目标：转化后向量里每个元素保持比较好的线性关系&lt;/li&gt;
          &lt;li&gt;离散化方法
            &lt;ol&gt;
              &lt;li&gt;等距离离散&lt;/li&gt;
              &lt;li&gt;等样本点离散&lt;/li&gt;
              &lt;li&gt;画图观察趋势（趋势、拐点）&lt;/li&gt;
            &lt;/ol&gt;
          &lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;方法2: 函数变换
        &lt;ol&gt;
          &lt;li&gt;通过非线性函数变换得到新的特征加入模型训练（需要对新加入特征做归一化)&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;方法3: 决策树里散发（切单采用）
        &lt;ol&gt;
          &lt;li&gt;决策树可以理解为一堆的 if … else …&lt;/li&gt;
          &lt;li&gt;天生可以对连续特征进行分段&lt;/li&gt;
          &lt;li&gt;gmail 在对信件重要性排序使用了决策树离散化方法&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;方法4: 核方法
        &lt;ol&gt;
          &lt;li&gt;可以理解为特征函数变换的一种方式&lt;/li&gt;
          &lt;li&gt;把核函数看成相似度的话， 则变成 KNN 模型或者加权平均模型&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;特征工程方法论
    &lt;ol&gt;
      &lt;li&gt;避免简单粗暴的归一化或者标准化，往往风险大于收益&lt;/li&gt;
      &lt;li&gt;尝试随机森林或者其他集成学习树模型暴力处理
        &lt;ol&gt;
          &lt;li&gt;决策树原型的模型对特征变量取值范围不敏感&lt;/li&gt;
          &lt;li&gt;有类似良好特性的分类器还包括:
            &lt;ol&gt;
              &lt;li&gt;特定种类的深度网络&lt;/li&gt;
              &lt;li&gt;L1范数正则化后的线性模型&lt;/li&gt;
            &lt;/ol&gt;
          &lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;移除不必要数据，降低变量维度
 在对特征做各种维度变化和复杂处理前，可以去掉无用和低贡献度的变量，降低后续处理难度
        &lt;ol&gt;
          &lt;li&gt;移除单一取值变量&lt;/li&gt;
          &lt;li&gt;移除低方差变量
            &lt;ol&gt;
              &lt;li&gt;和单一取值类似，虽然取值不唯一，但整体变化很小。&lt;/li&gt;
              &lt;li&gt;可人为设定阈值来去除该类型变量&lt;/li&gt;
            &lt;/ol&gt;
          &lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;转化描述变量
在不假设分类器的前提下，必须对描述变量转化为数字类型变量，因为大部分算法无法直接处理描述变量
        &lt;ol&gt;
          &lt;li&gt;连续特征(continuous)
            &lt;ol&gt;
              &lt;li&gt;归一化（去中心、方差归一）&lt;/li&gt;
            &lt;/ol&gt;
          &lt;/li&gt;
          &lt;li&gt;无序变量(特征)（categorical)
            &lt;ol&gt;
              &lt;li&gt;独热编码(one-hot、One-of-k),比如 color 取值为:
                &lt;ol&gt;
                  &lt;li&gt;red   (1,0,0)&lt;/li&gt;
                  &lt;li&gt;green (0,1,0)&lt;/li&gt;
                  &lt;li&gt;blue  (0,0,1)&lt;/li&gt;
                &lt;/ol&gt;
              &lt;/li&gt;
            &lt;/ol&gt;
          &lt;/li&gt;
          &lt;li&gt;有序变量(特征)（ordinal)
            &lt;ol&gt;
              &lt;li&gt;类似one-hot,包含顺序特征,比如status取值:
                &lt;ol&gt;
                  &lt;li&gt;bad      (1,0,0)&lt;/li&gt;
                  &lt;li&gt;normal   (1,1,0)&lt;/li&gt;
                  &lt;li&gt;good     (1,1,1)&lt;/li&gt;
                &lt;/ol&gt;
              &lt;/li&gt;
            &lt;/ol&gt;
          &lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Mon, 24 Sep 2018 00:00:00 +0800</pubDate>
        <link>/2018/09/%E5%88%87%E5%8D%95%E9%A1%B9%E7%9B%AE%E5%B0%8F%E7%BB%93/</link>
        <guid isPermaLink="true">/2018/09/%E5%88%87%E5%8D%95%E9%A1%B9%E7%9B%AE%E5%B0%8F%E7%BB%93/</guid>
        
        
      </item>
    
      <item>
        <title>词向量</title>
        <description>&lt;h3 id=&quot;词向量相关知识整理&quot;&gt;词向量相关知识整理&lt;/h3&gt;
&lt;hr /&gt;
&lt;h4 id=&quot;背景&quot;&gt;背景&lt;/h4&gt;
</description>
        <pubDate>Tue, 18 Sep 2018 00:00:00 +0800</pubDate>
        <link>/2018/09/%E8%AF%8D%E5%90%91%E9%87%8F/</link>
        <guid isPermaLink="true">/2018/09/%E8%AF%8D%E5%90%91%E9%87%8F/</guid>
        
        
      </item>
    
      <item>
        <title>从零搭建酒店类垂直搜索系统</title>
        <description>&lt;h3 id=&quot;前言&quot;&gt;前言&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;13年有机会帮前东家重构了搜索系统，一直忙忙碌碌，也没回头梳理一下，这里希望能把自己的实践和思考记录下来&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;背景&quot;&gt;背景&lt;/h3&gt;

&lt;h3 id=&quot;选型&quot;&gt;选型&lt;/h3&gt;

&lt;h3 id=&quot;架构&quot;&gt;架构&lt;/h3&gt;

&lt;h3 id=&quot;策略&quot;&gt;策略&lt;/h3&gt;
</description>
        <pubDate>Tue, 18 Sep 2018 00:00:00 +0800</pubDate>
        <link>/2018/09/%E4%BB%8E%E9%9B%B6%E6%90%AD%E5%BB%BA%E9%85%92%E5%BA%97%E7%B1%BB%E5%9E%82%E7%9B%B4%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/</link>
        <guid isPermaLink="true">/2018/09/%E4%BB%8E%E9%9B%B6%E6%90%AD%E5%BB%BA%E9%85%92%E5%BA%97%E7%B1%BB%E5%9E%82%E7%9B%B4%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/</guid>
        
        
      </item>
    
  </channel>
</rss>
