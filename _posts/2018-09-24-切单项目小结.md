### 背景
1. 场站交易线下交易严重。带来了如下问题
    1. 平台收益受损； 
    2. 乘客体验差;
    3. 容易引发线下超载等安全事故
2. 场站切单率.预计至少超过20%
    1. 客服随机电话回访
    2. 场站订单司机行为分析

### 思路
1. 根据司机历史统计行为进行建模
2. 司机在刷订单列表时，根据模型（司机特征、乘客特征、订单特征）预测切单概率
3. 对切单倾向严重的匹配做管控
    1. 最严重的直接过滤
    2. 其他司机按照其平均成交金额做分级管控
    3. 根据实时供需数据随时调整阈值（doing.
    4. 从安全角度应该直接清除，从增长角度需要区分场景
        1. 运力不足：适当放松阈值，优先满足乘客乘坐，同时平台有较低概率能够拿到提成
        2. 运力充足：适当收紧阈值，优先让平台上的优质司机(切单概率低、忠诚度高.接单

### 特征工程 
1. 数据清洗
    1. 数据质量
        1. 数据完整性--例如人的属性缺少性别、籍贯、年龄等
            1. 数据补全（剔除）
        2. 数据唯一性--不同来源数据出现重复情况
            1. 去除重复记录
        3. 数据权威性--同一个指标出现多个来源的数据，且数值不一样
            1. 选用最权威渠道数据
        4. 数据合法性--获取的数据与尝试不符，如年龄大于200岁
            1. 设定判定规则（或者半人工处理）
        5. 数据一致性--不同来源的不同指标，实际内涵是一样的（或统一指标内涵不一致）
            1. 建立数据体系
    2. 数据更适合挖掘
        1. 高纬度
            1. 降维（主成分分析、随机森林）
        2. 维度低
            1. 抽象
                1. 汇总（平均、加总、最大、最小）
                2. 离散化、聚类、自定义分组
        3. 无关信息和字段冗余
            1. 剔除
        4. 多指标数值、单位不同
            1. 归一化（最小-最大、零-均值、小数定标）
2. 缺失值处理（https://www.zhihu.com/question/26639110）
    1. 删除。最简单最直接的方法，很多时候也是最有效的方法，这种做法的缺点是可能会导致信息丢失。
        1. 删除有缺失数据的样本
        2. 删除有过多缺失数据的特征
    2. 补全。
        1. 用规则或模型将缺失数据补全，这种做法的缺点是可能会引入噪声。平均数、中位数、众数、最大值、最小值、固定值、插值等等
        2. 建立一个模型来“预测”缺失的数据。（KNN, Matrix completion等方法）
            1. 根本缺陷：如果其他变量与缺失变量无关，则预测结果无意义；反之如果预测结果非常准确，则说明加入这个变量无意义；
            2. 一般情况下，介于二者之间
        3. 引入虚拟变量(dummy variable.来表征是否有缺失，是否有补全。
    3. 忽略。
        1. 有一些模型，如随机森林，自身能够处理数据缺失的情况，在这种情况下不需要对缺失数据做任何的处理，这种做法的缺点是在模型的选择上有局限。
    4. 把变量映射到高维空间。
        1. 好处：保留了原始数据全部信息，不用考虑缺失值、不用考虑线性不可分之类问题
        2. 缺点：计算量太大，而且需要样本量足够大
        3. 举例：性别有男、女、缺失三种情况，映射为三个变量：是否男、是否女、是否缺失。连续型变量也可以如此处理。
    5. 缺失值较多
        1. 修改为  "XX字段有值"、"字段为空"
3. 特征转化
    1. **对原始特征转化，把原来的非线性关系转化为线性关系**
        1. LinearReg模型是线性模型，在非线性情况下效果不好
        2. 复杂模型(如Svm)对于高维特征有时间约束
        3. CTR 预估目前最常用方法还是 LR
    2. 方法1:离散化
        1. 目标：转化后向量里每个元素保持比较好的线性关系
        2. 离散化方法
            1. 等距离离散
            2. 等样本点离散
            3. 画图观察趋势（趋势、拐点）
    3. 方法2: 函数变换
        1. 通过非线性函数变换得到新的特征加入模型训练（需要对新加入特征做归一化)
    4. 方法3: 决策树里散发（切单采用）
        1. 决策树可以理解为一堆的 if … else …
        2. 天生可以对连续特征进行分段
        3. gmail 在对信件重要性排序使用了决策树离散化方法
    5. 方法4: 核方法
        1. 可以理解为特征函数变换的一种方式
        2. 把核函数看成相似度的话， 则变成 KNN 模型或者加权平均模型
4. 特征工程方法论
    1. 避免简单粗暴的归一化或者标准化，往往风险大于收益
    2. 尝试随机森林或者其他集成学习树模型暴力处理
        1. 决策树原型的模型对特征变量取值范围不敏感
        2. 有类似良好特性的分类器还包括:
            1. 特定种类的深度网络
            2. L1范数正则化后的线性模型
    3. 移除不必要数据，降低变量维度
        在对特征做各种维度变化和复杂处理前，可以去掉无用和低贡献度的变量，降低后续处理难度
        1. 移除单一取值变量
        2. 移除低方差变量
            1. 和单一取值类似，虽然取值不唯一，但整体变化很小。
            2. 可人为设定阈值来去除该类型变量
    1. 转化描述变量
       在不假设分类器的前提下，必须对描述变量转化为数字类型变量，因为大部分算法无法直接处理描述变量
        1. 连续特征(continuous)
            1. 归一化（去中心、方差归一）
        2. 无序变量(特征)（categorical)
            1. 独热编码(one-hot、One-of-k),比如 color 取值为:
                1. red   (1,0,0)
                2. green (0,1,0)
                3. blue  (0,0,1)
        3. 有序变量(特征)（ordinal)
            1. 类似one-hot,包含顺序特征,比如status取值:
                1. bad      (1,0,0)
                2. normal   (1,1,0)
                3. good     (1,1,1)