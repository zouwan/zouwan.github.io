<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>
### 1. 什么是感知机(Perception)？
1. <b>感知机</b>(perceptron)是二类分类的线性分类模型,其输入为实例的特征向量,输出为实例的类别,取+1和-1二值.
* 感知机对应于输入空间中将实例划分为正负两类的分离超平面，属于 <b>判别模型</b>。
* 感知机学习旨在求出将训练数据进行线性划分的分离超平面，为此导入了基于误分类的损失函数，利用梯度下降法对损失函数进行极小化，求得感知机模型。
* 感知机学习算法具有简单而易于实现的优点，分为原始形式和对偶形式。
* 感知机是神经网络与支持向量机的基础。

### 2. 感知机模型原理
感知机满足输入空间到输出空间的如下函数称为感知机：
    $$f(x)=sign(wx+b)$$ 其中 w 称为权值(weight，b 称为偏置(bias), sign为激活函数
    $$sign(x)=\begin{cases} +1,x>0 \\ -1,x<0 \\ \end{cases}$$
   sign(x) 将大于0的分为1， 小于0的分为-1.
0. 感知机相比逻辑回归主要区别是 <b>激活函数不同</b>
    1. 感知机激活函数是 sign(x),又称单位阶跃函数
    2. 逻辑回归（logistic regression)激活函数是 sigmoid
    3. sigmoid一般取阈值0.5， 大于0.5的分为1， 小于0.1的分为0
    4. 逻辑回归也被看做是一种概率估计， 详见逻辑回归(todo)
1. 工作原理
    1. 给每一个属性一个权重，对属性和权重乘积求和得到的结果和一个阈值进行比较，可以输出一个二分类结果。比如判定是否能够给张三放贷。
2. 感知机的几何解释:
    1. 感知机可以用线性方程表示；
    $$ w \cdot x+b=0$$
    1. 几何意义如下图:
    对应于特征空间Rn中的一个超平面S，其中w是超平面的法向量，b是截距。这个超平面将特征空间划分为两个部分，位于两部分的点分别被分为正、负两类。因此，超平面S称为分离超平面（separating hyperplanes）
    ![image](https://pic2.zhimg.com/80/v2-7bd3d267a3d50a511b4b51aace348301_hd.png)
    3. 啥？超平面 （黑脸问号)
        1. 超平面在二维空间里就是直线，方程是a*x+b*y+c=0
        2. 超平面在三维空间里就是平面，方程是a*x+b*y+c*z+d=0
        3. 在n维空间里推广就是就是a*x+b*y+c*z+........+k=0
           (a,b,c...)对应向量w, 是由平面确定的数，
          （x,y,z..)是平面上任一点的坐标，对应方程的向量x    
    4. 假定 $wx+b=0$是一个超平面，令$g(x)=wx+b$,即超平面上的所有点都满足$g(x)=0$. 对于超平面的一侧点满足 $g(x)>0$, 另一侧满足 $g(x)<0$.
    5. 对于不在超平面上的点x到超平面的距离是 $ r=\frac {g(x)}{\left\|w \right\|}, w为L2范数$
        证明：见下图，其中 O 为原点， $X_p$为超平面上的点， X 是超平面外的点， w 为超平面法向量
![image](https://img-blog.csdn.net/20171015173343056?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSmFzdGVyX3dpc2RvbQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

     (1) $X=X_p+r * \frac{w}{ \left\|w\right\|}$
>向量基本运算法则：$ OX = OX_p + X_pX$; w为法向量，所以 $\frac{w}{\left\|w\right\|}$ 是垂直于超平面的单位向量
  
     (2) $g(x) = w^T(x_p+r\frac{w}{\left\|w\right\|})+w_0 = (w^T * x_p + w_0) + r * \frac{w^T * w}{\left\|w\right\|} = r * \left\| w \right\|$
>等式（1）带入等式 $g(x)=wx+b$, 由于$X_p$在超平面， 所以 $g(X_p) = w^T * x_p + w_0 = 0$
  
        结论一得证
  
### 3. 感知机学习目标
&emsp;&emsp;学习参数w与b，确定了w与b，图上的直线（高维空间下为超平面）也就确定了; 感知机的学习目标是找到损失函数,转化为最优化问题
1. 思路一：误分类点数目
    1. 无法函数化表示
    2. w, b 不是连续可导
    3. 由于上述1，2两点，思路一不可行，考虑思路二
2. 思路二：误分类点距离超平面距离，总距离越小越好
    $$ \frac{1}{\| w\|} | w \cdot x_0 + b | $$
    1. 当数据点被误分类(+1吧被误判为-1), 则 $(w \cdot x_0 + b) < 0 $, 此时预测值 $y_i$=+1, 满足 $$-y_i(w \cdot x_0 + b) > 0$$
    2. 当数据点被误分类(-1吧被误判为+1), 则 $(w \cdot x_0 + b) > 0 $, 此时预测值 $y_i$=-1, 满足 $$y_i(w \cdot x_0 + b) > 0 $$
3. 损失函数确定
    1. 上式不考虑$\|w\|$, 去掉后面的绝对值，得到损失函数:
    $$ J(w,b) = -\sum_{x_i\in M}{y_i(w \cdot x_i + b)}，M为误分类点集 $$
    
    1. 感知机不关心实际超平面距离每个点的距离（定量，误分类点的距离），只关心最终分类是否正确（定性，误分类点的个数）
1. 损失函数推导：
    1. 把正例预测为负例： $w*x_i+b>0时， y_i=-1$
    2. 把负例预测为正例： $w*x_i+b<0时， y_i=1$
    3. 可以得到误分类点到超平面总的距离是（注意， 只考虑不在超平面的点）
        $$-\frac{1}{\left\|w\right\|}\sum_{x_i \in M}{y_i*(w*x_i+b)}$$ 
        其中$\left\|w\right\|$可以忽略（原因1：$frac{1}{\left\|w\right\|}$ 不影响正负的判断，即不影响学习算法的中间过程我们；原因2：感知机训练终止条件是所有样本正确分类, 即不存在误分类点，因此$frac{1}{\left\|w\right\|}$ 对最终结果无影响），得到损失函数：
        $$-\sum_{x_i \in M}{y_i*(w*x_i+b)}$$

2. 更新策略（梯度下降）
    1. 对损失函数计算梯度（梯度下降）
$$\left\{ \begin{array}{lr} \nabla_{w}{L(w,b)} = -\sum_{x \in M}{y_i * x_i} \\ \nabla_{b}{L(w,b)} = -\sum_{x \in M}{y_i} \end{array} \right. $$<br>
这里使用随机梯度（随机选取误分类点$(x_i, y_i)$对 w, b 进行更新， 更新公式如下：
$$\left\{ \begin{array}{lr} w \leftarrow w+\eta y_i*x_i \\ b \leftarrow b+\eta y_i  \end{array} \right. $$
    2. 如何理解上面更新公式？(from [知乎](https://www.zhihu.com/question/57747902))
        1. 梯度更新公式确实不是推导而是创造出来的，所以只能从概念上去理解
        2. 设想下有个函数，你的目标是：找到一个参数$\theta$ 使得它的值$Y$最小。但它很复杂，你无法找到这个参数的解析解，所以你希望通过梯度下降法去猜这个参数
        3. 问题是怎么猜？
        对于多数有连续性的函数来说，显然不可能把每个$\theta$都试一遍。所以只能先随机取一个$\theta$，然后看看怎么调整它最有可能使得$\theta$变小。
        把这个过程重复n遍，自然最后得到的损失函数L的估值会越来越小
        4. 调整策略（参考文章:[梯度下降法](https://zouwan.github.io/2018/11/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/)）
        基于当前我们拥有的那个参数$\theta$，所以有了：$$\theta_{t+1}=\theta_t + \Delta$$那现在问题是每次更新的时候这个$\theta$应该取什么值？
        我们知道关于某变量的（偏）导数的概念是指当（仅仅）该变量往 <b>正向</b> 的变化量趋向于0时的其函数值变化量的极限。若求Y关于$\theta_t$的导数，得到一个值比如+5, 那就说明若现在我们把$\theta_t$往负向移动，损失函数 $J(w,b）$的值会变小，但不一定是正好-5。同理若现在导数是-5，那么把$\theta$往 <b>负向</b>移动，损失函数$J(w,b)$值会变小。  
        不管导数值$\theta$是正的还是负的(正负即导数的方向), 对于$J(w,b)$来说，$-\theta$的最终方向（即最终的正负号，决定是增(+)还是减(-))一定是能将$J(w,b)$值变小的方向（除非导数为0）。所以有了:
        $$\theta_{t+1}=\theta_t + (-\Delta)$$但是说到底，$\Delta$的绝对值只是个关于Y的变化率，本质上和$\theta_t$没关系。所以为了抹去$\Delta$在幅度上对$\theta_t$的影响，需要一个学习率来控制：$\alpha$。所以有了：$\alpha \in \left(0,1\right]$
        $$\theta_{t+1}=\theta_t + (-\alpha \Delta) =\theta_t -\alpha \Delta$$就是有多少个参数，就有多少个不同的$\Delta$。
        <br>现在分析在梯度下降法中最常听到的一句话：“梯度下降法就是朝着梯度的反方向迭代地调整参数直到收敛。” 这里的梯度就是 $\Delta$ ,而梯度的反方向就是 $-\Delta$ 的符号方向---梯度实际上是个向量。所以这个角度来说，即使我们只有一个参数需要调整，也可以认为它是个一维的向量。 
        <br<b>整个过程你可以想象自己站在一个山坡上，准备走到山脚下（最小值的地方），于是很自然地你会考虑朝着哪个方向走，方向由$\Delta$的方向给出，而至于一次走多远，由$|\alpha\Delta|$来控制。这种方式相信你应该能理解其只能找到局部最小值，而不是全局的。</b>
        1. 换个角度看，想象损失函数是$y=x^2$的抛物线形式，其梯度为 $y'=2x$
            1. 纵坐标y表示损失函数，横坐标x表示参数 $\theta$
            2. $\theta$在最低点左边时，导数小于0，只有$\theta$往右边（最低点方向）走，损失函数才会变小，即$\theta$要往导数的负方向变化
                1. 当 x=-2; y=-4, 此时往负向损失函数$y$增大，往正向走损失函数$y$减少
            3. $\theta$在最低点右边时，导数大于0，只有$\theta$往左边（最低点方向）走，损失函数才会变小，即$\theta$要往导数的负方向变化
                1.当 x=+2; y=+4, 此时往正向损失函数$y$增大，往负向走损失函数$y$减少            
                ![-w450](/assets/images//15432151219295.jpg)


    3. 更新方法
        <br>对于感知机模型 $f(x)=sign(w*x+b)$
        1. 选取初值 $w_0, b_0$
        2. 在训练集选择数据 $(x_i, y_i)$  
            1. 任意抽取数据点，当所有数据点没有误分类时结束算法，否则进入3.
        3. 此时有 $y_i(w*x_i+b)<0$ ， 更新参数:
            $\left\{ \begin{array}{lr} w \leftarrow w+\eta y_i*x_i \\ b \leftarrow b+\eta y_i  \end{array} \right. $
        1. 其中$\eta$ 学习率一般为0-1之间
    1. 来段代码解析 
        1. 例题来自李航《统计学习方法》的感知机部分
        ![](https://pic1.zhimg.com/80/v2-0575c3efd5d8e1322aaf1d6ef18808ac_hd.jpg)
        1. 例题解析
            1. 构建优化函数： $min_{w,b}J(w,b) = -\sum_{x_i \in M}y_i(w \cdot x + b)$
            2. 求解w, b, 为了计算方便，设 $\eta=0.1$ （$\eta$为学习率，建议设置 0-1之间）
        2. 步骤
                取初值: w=[0 0] b=1
                0:w=[0 0] b=1x=[3 3] 预测正确,跳过
                0:w=[0 0] b=1x=[4 3] 预测正确,跳过
                0:w=[0 0] b=1x=[1 1] 预测错误,更新w,b
                i=0 w=[-1 -1] b=0
                
                1:w=[-1 -1] b=0x=[3 3] 预测错误,更新w,b
                i=1 w=[2 2] b=1
                
                2:w=[2 2] b=1x=[3 3] 预测正确,跳过
                2:w=[2 2] b=1x=[4 3] 预测正确,跳过
                2:w=[2 2] b=1x=[1 1] 预测错误,更新w,b
                i=2 w=[1 1] b=0
                
                3:w=[1 1] b=0x=[3 3] 预测正确,跳过
                3:w=[1 1] b=0x=[4 3] 预测正确,跳过
                3:w=[1 1] b=0x=[1 1] 预测错误,更新w,b
                i=3 w=[0 0] b=-1
                
                4:w=[0 0] b=-1x=[3 3] 预测错误,更新w,b
                i=4 w=[3 3] b=0
                
                5:w=[3 3] b=0x=[3 3] 预测正确,跳过
                5:w=[3 3] b=0x=[4 3] 预测正确,跳过
                5:w=[3 3] b=0x=[1 1] 预测错误,更新w,b
                i=5 w=[2 2] b=-1
                
                6:w=[2 2] b=-1x=[3 3] 预测正确,跳过
                6:w=[2 2] b=-1x=[4 3] 预测正确,跳过
                6:w=[2 2] b=-1x=[1 1] 预测错误,更新w,b
                i=6 w=[1 1] b=-2
                
                7:w=[1 1] b=-2x=[3 3] 预测正确,跳过
                7:w=[1 1] b=-2x=[4 3] 预测正确,跳过
                7:w=[1 1] b=-2x=[1 1] 预测错误,更新w,b
                i=7 w=[0 0] b=-3
                
                8:w=[0 0] b=-3x=[3 3] 预测错误,更新w,b
                i=8 w=[3 3] b=-2
                
                9:w=[3 3] b=-2x=[3 3] 预测正确,跳过
                9:w=[3 3] b=-2x=[4 3] 预测正确,跳过
                9:w=[3 3] b=-2x=[1 1] 预测错误,更新w,b
                i=9 w=[2 2] b=-3
                
                10:w=[2 2] b=-3x=[3 3] 预测正确,跳过
                10:w=[2 2] b=-3x=[4 3] 预测正确,跳过
                10:w=[2 2] b=-3x=[1 1] 预测错误,更新w,b
                i=10 w=[1 1] b=-4
                
                11:w=[1 1] b=-4x=[3 3] 预测正确,跳过
                11:w=[1 1] b=-4x=[4 3] 预测正确,跳过
                11:w=[1 1] b=-4x=[1 1] 预测正确,跳过
                w=[1 1] b= 第11 次更新时得到解
        1. 代码
           李航《统计学方法》p29 例2.1, 正例：x1=(3,3), x2=(4,3),负例：x3=(1,1)
                
             ```
                import numpy as np
                import matplotlib.pyplot as plt
                # x取值，样本数据，对应的是一个二维向量
                p_x = np.array([[3, 3], [4, 3], [1, 1]])
                # y取值，样本数据对应的正负标签，-1=负样本， +1=正样本
                y = np.array([1, 1, -1])
                
                # 输出样本数据，其中正样本为红色，负样本为蓝色
                plt.figure()
                for i in range(len(p_x)):
                    if y[i] == 1:
                        plt.plot(p_x[i][0], p_x[i][1], 'ro')
                    else:
                        plt.plot(p_x[i][0], p_x[i][1], 'bo')
                w = np.array([0, 0])
                b = 1
                delta = 1
                 
                print("取初值: w=" + str(w) + " b=" + str(b))
                for i in range(100):
                    choice = -1
                    for j in range(len(p_x)):
                        y_pred = np.sign(np.dot(w, p_x[j]) + b)
                        if y[j] != y_pred:
                            print(str(i)+":w="+str(w)+" b="+str(b)+"x="+str(p_x[j])+" 预测错误,更新w,b")
                            choice = j
                            break
                        else:
                            print(str(i)+":w="+str(w)+" b="+str(b)+"x="+str(p_x[j])+" 预测正确,跳过")
                
                    if choice == -1:
                        print("w=" + str(w) + " b=" + " 第" + str(i) + " 次更新时得到解")
                        break
                    w = w + delta * y[choice]*p_x[choice]
                    b = b + delta * y[choice]
                    print("i=" + str(i) + " w=" + str(w) +" b=" + str(b) + "\n")
                 
                line_x = [0, 10]
                line_y = [0, 0]
                 
                for i in range(len(line_x)):
                    line_y[i] = (-w[0] * line_x[i]- b)/w[1]
                 
                plt.plot(line_x, line_y)
            
                ```
        
        
        1. 输出
![](/assets/images//15432216556174.jpg)

        